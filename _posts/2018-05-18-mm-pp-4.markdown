---
layout: post
title:  "Notes on Probability Primer 4: Expectations, etc."
date:   2018-05-18 00:00:00
author: 장승환
categories: Notes
tags: Probability
---

#### (PP 4.1) Expectation for discrete random variables

("Average value")

Let $\mathscr{X} = X(\Omega)$.

**Definition.** The *expectation* of a random variable $X$ with PMF $p$ is

$$\mathbb{E}(X) = \sum_{x \in \mathscr{X}} xp(x)$$

when this sum is 'well-defined.'
Otherwise, the expactation does not exist.

"*Well-defined* means well-deinfed as a sum of infinite sereis."

**Definition.**
Let $a_1, a_2, \ldots \in \mathbb{R}$ and 
let $\sum_{i=1}^\infty a_i = \sum_{i: a_i \ge 0}a_i + \sum_{i: a_i < 0}a_i = b+c$.  
$\sum_{i=1}^\infty a_i$ is *well-defined* if either $b$ or $c$ is finite.

**Example.**
et $c = \sum_{k = 1}^\infty \frac{1}{k^2} < \infty$   
* $\mathbb{E}(X)$ can be inifinite: let $p(k) = \frac{1}{ck^2}$ for $k \in \\{1, 2, \ldots \\}$  
$$\mathbb{E}(X) = \sum_{i=1}^\infty \frac{k}{ck^2} = \sum_{k=1}^\infty = \infty$$
* $\mathbb{E}(X)$ might not exists: $p(k) = \frac{1}{ck^2}$ for $k \in \mathbb{Z} -\\{0\\}$
$$\mathbb{E}(X) = \sum_{i= 1}^\infty = \sum_{k\ge 0}+ \sum_{<0} = \infty -\infty$$ (undefined)

---

#### (PP 4.2) Expectation for random variables with densities

**Definition.** The *expectation* of a random variable $X$ with density $f$ is

$$\mathbb{E}(X) = \int_{-\infty}^\infty xf(x)dx$$

when this integral is 'well-defined.'
Otherwise, the expactation does not exist.

**Example.**
$X \sim {\rm Uniform}(a,b)$  
$$\mathbb{E}(X) = \int_a^b\frac{x}{b-a}dx = \frac{1}{b-a} \frac{x^2}{2}\vert_a ^b=\frac{1}{b-a}\frac{1}{2}(b^2-a^2)=\frac{a+b}{2}$$

---

#### (PP 4.3) Expectation rule

**Fact.** $g(X)$ is a random variable and $g: \mathbb{R} \rightarrow \mathbb{R}$ is measurable.

**Theorem. (Expectation rule)**
If $X is a random variable and $g: \mathbb{R} \rightarrow \mathbb{R}$ is measurable, then
1. $\mathbb{E}(g(X)) = \sum_{x \in \mathscr{X}}g(x)p(x)$ if $X$ is discrete with PMF $p$
2. $\mathbb{E}(g(X)) = \int_{-\infty}^\infty g(x)p(x)$ if $X$ is discrete with PMF $p$  
(when these quantities are well-defined)

---

#### (PP 4.4) Properties of expectation

---

#### (PP 4.5) Mean, variance, and moments

---

#### (PP 5.1) Multiple discrete random variables

**Definition.** Given $(\Omega, \mathscr{A}, P)$, a <span style="color:red">*random vector*</span> is a measurable function 

$$X : \Omega \rightarrow \mathbb{R}^d$$

where $d \in \mathbb{N}$.

**Definition.** A *discrete random vector* $X \in \mathbb{R}^d$ is s.t. $X(\Omega)$ is countable.

**Definition.** The (joint) PMF (or joint distribution) of a discrete random vector $X \in \mathbb{R}^d$ is the function

$$p:\mathbb{R}^d \rightarrow [0,1]$$ 

such that $p(x)= P[X=x]$ for all $x \in \mathbb{R}^d$. 

**Notation.** $X = (X_1, \ldots, X_d)$, $x = (x_1, \ldots, x_d)$  
($X = x$ means $X_i = x_i$ for all $i$)  
$p(x) = p(x_1, \ldots, x_d)$  
$p_X(x) = p(x)$  

**Remark.** 

$$P[X \in A] = \sum_{x\in \mathscr{X}, x \in A} p(x)$$

where $\mathscr{X} = X(\Omega)$.

**Reamrk.** $g(X)$ is a random vector if $X \in \mathbb{R}^d$ is a random variable and
$g: \mathbb{R}^d \rightarrow \mathbb{R}^k$ is measurable.

**Proposition.**

$$\mathbb{E}(g(X)) = \sum_{x\in \mathscr{X}} g(x)p(x)$$

for any measurable $g: \mathbb{R}^d \rightarrow \mathbb{R}$ such that this sum is well-defined.

---

#### (PP 5.2) Marginals and conditionals

Deal with 2-dimensional case only.

Fix $(X, Y) \in \mathbb{R}^2$ with $p(x, y) = P[X=x, Y=y]$.

**Definition.** The *marginal PMF* of $X$ is $p_X(x) = P[X = x]$.

**Proposition.**

$$p_X(x) = \sum_{y \in Y} p(x, y)$$

Proof. $p_X(x) = P[X=x] = \sum_{y \in \mathscr{Y}} P[X=x, Y=y] = \sum_{y \in \mathscr{Y}}p(x,y)$  
noting $\\{\omega \in \Omega : X(\omega) = x\\} 
= \cup_{y \in \mathscr{Y}}\\{\omega \in \Omega : X(\omega) = x, Y(\omega) = y\\}$ 

**Notation.** $p(x) = p_X(x)$, $p(y) = p_Y(y)$  

**Definition.** The *conditional PMF* of $X$ given $Y=y$ is 

$$p(x\vert y) = P[X=x \vert Y = y]$$

(when $P[Y=y] > 0$.)

**Remark.** 

$$p(x\vert y) = \frac{P[X=x, Y=y]}{P[Y=y]} = frac{p(x, y)}{p(y)}$$

**Definition.** The conditional expectation of $X$ given $Y=y$ (when $p(y) > 0$) is 

$$\mathbb{E}(X\vert Y=y) = \sum_{x \in \mathscr{X}} xp(x \vert y)$$

when this sum is well-defined.

**Remark.** $\mathbb{E}(X\vert Y)$ is a random variable that depends on the random variable $Y$.

---

$$ $$

*To be added..*

---

