---
layout: post
title:  "MDP"
date:   2018-03-01 00:00:00
author: 장승환
categories: 기계학습
tags: 강화학습 RL
---

논의를 시작하기 전에 지난번 살펴보았던 확률변수의 개념에 대해 몇 가지 언급해야 할 사항이 있다.


첫째, 지난 포스트 [/확률변수를 이해하다/](https://cveai.github.io/%EA%B8%B0%EA%B3%84%ED%95%99%EC%8A%B5/2018/02/14/rvariable.html)에서는 
이산 확률변수 중에서도 표본공간이 **유한**(finite)인 경우만을 다루었다. 
유한인 경우의 모든 내용이 무한이면서 이산인 경우로 일반화되는데, 달라지는 거의 유일한 포인트는 극한 개념을 도입해야 한다는 것이다. 
극한의 개념이 요구되는 이유를 두가지로 생각해볼 수 있다. 
* 확률변수의 기반이 되는 표본공간이 무한일 경우. 
* 새로운 확률변수를 무한개 확률변수의 합으로 정의할 때.

표본공간이 무한인 이산의 경우는 본질적으로 각 항이 0보다 크거나 같고 $(a_n \ge 0)$ 급수가 1로 수렴하는 $(\sum s_n = 1)$ 무한수열 $a_n$을 제시하는 것과 동일하다.
여기서 $f(n) = a_n$ 으로 정의하면 $\Omega = \\{1, 2, ... \\}$를 표본공간으로 하는 확률분포함수를 얻게된다.

<figure>
<img src="/assets/pics/mdp/one_over_n.png" alt="Dice" style="width: 80%; height: 80%">
<figcaption>무한 이산 확률분포함수 (using: <a href="https://www.desmos.com/calculator/2kmx0enkkz">Desmos online calculator</a>)
</figcaption>
</figure>

이 확률분포함수에 관한 확률변수 $X: \{1, 2, \ldots, \} \rightarrow \mathbb{R}$를 생각해보면 달라지는 점은 $X$의 정의구역인 표본공간 $\Omega$이 무한이라는 점이다.

무한개의 활률변수를 합하는 경우는 MDP의 개념을 이해해가면서 구체적으로 다루기로 하자. 

둘째, 확률변수는 확률적 성격 혹은 불확실성을 가진 데이터 (측정에 불확실성도 포함)(의미) --> 함숫값이 실수가 아니어도 된다.

마지막으로, 연산 및 합성해도 확률변수
 
요약하자면,
* 무한인 경우도 이산일 경우 급수의 극한 걔념을 이용하여 유한의 경우와 매우 유사하게 확률변수를 다룰 수 있다.
* 수가 아니어도
* 덧셈 및 합성



#### 마코프 (Markov)



#### 마코프 모델 (Markov Model)

시간에 따라 변화하는 데이터를 수학적으로 기술할 때 (거의 필수적으로) 사용되는 것이 마코프 모델이다.
물론 시간순의 데이터를 모델링하는 중요한 이유 중 하나는 (미래에 대한) 예측이다.


#### MDP (MArkov Decision Process)

상황설명


확률변수로 바로 들어감



MDP의 개념은 다음의 다섯가지의 기본 요소를 명확히 이해하면 어렵지 않게 머릿속에 그림을 그릴 수 있다.

* Agent
* Environment
* Action
* State
* Reward 


MDP 모델을 조금 쉽게 이해하기 위해 두 주체 간의 게임으로 이해해보자.
이 게임은 주인공인 agent와 주인공이 모험을 할 수 있도록 배경을 설정해주는 environment 간의 상호작용이다.
게임은 이산적인 time step (편의상 시점이라고 부르겠다) 을 따라가며 상호작용이 단계적으로 이루어진다.
좀 더 정확하게 표현해보면 시점을 나타내는 파라미터 $t$가 $t=0$, $t=1$, $t=2$, $\ldots$ 이렇게 변해가면서
agent와 environment가 각자의 활동을 하며 상호작용을 이어가게 된다.
각 시점에서 agent가 할 수 있는 선택을 action이라고 한다.
Agent가 게임에서 취할 수 있는 모든 가능한 action을 다 모아놓은 집합을 $\mathscr{A}$라고 하고, 
시점 $t$에 취하는 action을 $A_t$라고 표시하자. 자연스럽게 $A_t \in \mathscr{A}$임을 알 수 있다.
반면, 각 시점에서 environment는 두가지 반응으로 agent의 action에 대응하는데 그것이 각각 state와 reward이다.
Action의 경우와 마찬가지로 시점 $t$에 environment가 내놓게 되는 state를 $S_t$, reward를 $R_t$로 표시한다.
마찬가지로 $\mathscr{S}$와 $\mathscr{R}$는 각각 가능한 state와 reward의 집합을 나타낸다.


#### 던전 키퍼

1997년에 출시된 [던전 키퍼](https://web.archive.org/web/20020425231256/http://dk2.ea-europe.com/uk/dk1/index.html)라는 조금 철지난 비디오 게임이 있다.
(철지난 줄 알았는데 모바일 버전으로 나와있네요 [[AppStore](https://itunes.apple.com/kr/app/%EB%8D%98%EC%A0%84-%ED%82%A4%ED%8D%BC/id659212037?mt=8), [Google Play](https://play.google.com/store/apps/details?id=com.ea.game.dungeonkeeper_row&hl=ko)].)
일반적인 게임과는 달리 이 게임에서 게이머는 영웅이 아닌 악의 축 역할을 맡게 된다. 게다가 어마어마한 마법의 능력을 가진.
게이머는 엄청난 악의 힘으로 지하 던전을 통제한다. 곳곳에 함정을 놓고 비밀 방을 만들고.. 영웅을 향해 갖가지 시련을 먼들어낸다.


#### MDP (Markov Decision Process)

자 이제까지의 내용을 종합하여 정리해보자.


#### 그리드 월드 (Grid world)



#### 참고자료

[1] R. Sutton, A. Barto, *Reinforcement leaning: an introduction*, second edition ([final draft](http://incompleteideas.net/book/the-book-2nd.html)).
[2] D. Silver, *RL Course, Lecture 2: Markov Decision Process* ([유튜브 비디오](https://youtu.be/lfHX2hHRMVQ))



[1] A. Shirayaev (translator: D. Chibisov), *Probability- 1*, third edition (2016), Springer.  
[2] 김민형, *확률론의 선과 악: 2. 확률론의 기원* ([네이버티비 동영상](http://tv.naver.com/v/1402550)).  
[3] Mathematical monk, *Probability primer* ([유튜브 동영상](https://www.youtube.com/watch?v=Tk4ubu7BlSk&list=PL17567A1A3F5DB5E4)).  
[4] PennStae, Eberly College of Science, *STAT 414: Probability theory* ([온라인 코스](https://onlinecourses.science.psu.edu/stat414/)).


---

*읽으시다 오류나 부정확한 내용을 발견하시면 꼭 알려주시길 부탁드립니다. 감사합니다.*  
(권 경모님, 권 휘님, 박 진우님, 손 주형님, 이 규복님, 김 기철님 감사드립니다.)


---

*읽으시다 오류나 부정확한 내용을 발견하시면 꼭 알려주시길 부탁드립니다. 감사합니다.*  
(권 경모님, 권 휘님, 박 진우님 감사드립니다.)
