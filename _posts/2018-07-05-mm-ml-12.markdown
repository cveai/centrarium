---
layout: post
title:  "Notes on Machine Learning 12: Model selection"
date:   2018-07-05 00:00:00
author: 장승환
categories: Notes
tags: ML
---

#### (ML 12.1) Model selection - introduction and examples

<span style="color:orange">**"Model" selection**</span> really means "complexity" selection!

Here, *complexity* $\approx$ flexibility to fit/explain data  

**Example** (Linaer regression with MLE for $w$)  $f(x) = w^T\varphi(x)$  
Given data $x \in \mathbb{R}$, consider polynomial basis $\varphi(x) = x^k$, $\varphi = (\varphi_0, \varphi_1, \ldots, \varphi_B)$

Turns out <span style="color:purple">$B =$ "complexity parameter"</span>

<figure>
<img src="/assets/pics/mm-ml/model-selection.png" alt="Model selection illustration" style="width: 80%; height: 80%">
<figcaption>Model (in fact complexity) selection illustration
</figcaption>
</figure>

**Example** (Bayesia linear regression or MAP)
<figure>
<img src="/assets/pics/mm-ml/bayesian-regression.png" alt="Bayesian linear regression" style="width: 100%; height: 100%">
<figcaption>
</figcaption>
</figure>

**Example** ($k$NN)   
<figure>
<img src="/assets/pics/mm-ml/knn-model-selection.png" alt="Classification example" style="width: 80%; height: 80%">
<figcaption>
</figcaption>
</figure>
<span style="color:purple">$k$</span> "controls" decesion boundaties.


---

#### (ML 12.2) Bias-variance in model selection

Bias-variance trade-off, as they say.  
MSE $=$ bias$^2 +$ var  
Can do the same for intervals: $\int$MSE $=$ $\int$bias$^2 +$ $\int$var  
(only applies for <span style="color:green">square loss</span>)
<figure>
<img src="/assets/pics/mm-ml/bias-var-convex.png" alt="Bias-variance trade-off" style="width: 60%; height: 60%">
<figcaption>Bias-variance "trade-off"
</figcaption>
</figure>

<figure>
<img src="/assets/pics/mm-ml/bias-var-trade.png" alt="Bias-variance trade-off" style="width: 80%; height: 80%">
<figcaption>
</figcaption>
</figure>

---

#### (ML 12.3) Model complexity parameters

**Complexitycontrolling parameters**
* not parameters used to fit the data
* In Bayesian models, tend to be "hyperparameters" (i.e. parameters of the prior)

---

#### (ML 12.4) Bayesian model selection

<<<<<<< HEAD
**Bayesian Model Averaging (BMA)** (just being fully Bayesian)

$p(y\vert x, \theta, m)$ where $\theta$ and $m$ are random variables

$p(y\vert x, D) = \inf p(y\vert x, \theta, m)p(m\vert x, D)dm$

where $p(y\vert x, D, m) = \int p(y\vert x, \theta, m)p(\theta \vert D, m)d\theta$

**Alternative: "Type II MAP$^\star$"**

$p(y\vert x, D) \approx p(y\vert x, D, m^\*)$ where $m^* \in \arg\max_mp(m\vert D)$

$p(m\vert D) \propto p(D\vert m)p(m)$

**cf. Type II ML/Evidence Approximation/Empirical Bayes :**

$p(y\vert x, D) \approx p(y\vert x, D, m^\*)$ where $m^* \in \arg\max_mp(D\vert m)$

where $p(D\vert m)$ is called the marginal likelihood

---

#### (ML 12.5) (ML 12.4) (ML 12.5) Cross-validation

---

#### (ML 14.1) Markov models - motivating examples
||||||| merged common ancestors
---

#### (ML 14.1) Markov models - motivating examples
=======
**Bayesian Model Averaging (BMA)** (just being fully Bayesian)
>>>>>>> notes

$p(y\vert x, \theta, m)$ where $\theta$ and $m$ are random variables

$p(y\vert x, D) = \inf p(y\vert x, \theta, m)p(m\vert x, D)dm$

where $p(y\vert x, D, m) = \int p(y\vert x, \theta, m)p(\theta \vert D, m)d\theta$

**Alternative: "Type II MAP$^\star$"**

$p(y\vert x, D) \approx p(y\vert x, D, m^\*)$ where $m^* \in \arg\max_mp(m\vert D)$

$p(m\vert D) \propto p(D\vert m)p(m)$

**cf. Type II ML/Evidence Approximation/Empirical Bayes :**

$p(y\vert x, D) \approx p(y\vert x, D, m^\*)$ where $m^* \in \arg\max_mp(D\vert m)$

where $p(D\vert m)$ is called the marginal likelihood

---

#### (ML 12.5) (ML 12.6) (ML 12.7) Cross-validation

Data: $D = ((x_1, y_1), \ldots, (x_n, y_n))$ iid\\
Models: $m \in \\{1, \ldots, C\\}$ \\
Error: $\varepsilon_m = {\rm EL}(Y, f_m(X))$ where $(X, Y) \sim p$ (true unknown distribution)

A. Validation
<figure>
<img src="/assets/pics/mm-ml/validation.png" alt="Validation" style="width: 80%; height: 60%">
</figure>

B. Cross-validation
<figure>
<img src="/assets/pics/mm-ml/cv-1.png" alt="Cross-Validation" style="width: 80%; height: 60%">
</figure>
<figure>
<img src="/assets/pics/mm-ml/cv-2.png" alt="Cross-Validation" style="width: 80%; height: 60%">
</figure>
(5) Retrain using $m^*$ on all of $D$.

C. LOOCV (Leave-One-Out CV) $k = n$

D. Random Subsamples
<figure>
<img src="/assets/pics/mm-ml/random-sub.png" alt="Random Subsampling" style="width: 80%; height: 60%">
</figure>

<figure>
<img src="/assets/pics/mm-ml/k.png" alt="Random Subsampling" style="width: 80%; height: 60%">
</figure>

---