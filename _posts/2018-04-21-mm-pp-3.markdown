---
layout: post
title:  "Notes on Probability Primer 3: Random variables"
date:   2018-04-21 00:00:00
author: 장승환
categories: Notes
tags: Probability
---

#### (PP 3.1) Random Variables - Definition and CDF

"Random variables are where the rubber meets the road in probability." -- Mathematical Monk

"Random variable is not necessarily random and it's not necesaarily a variable."   
-- [David Duvenaud](https://www.cs.toronto.edu/~duvenaud/)
(TWiML&AI: [Composing Graphical Models With Neural Networks](https://twimlai.com/twiml-talk-96-composing-graphical-models-neural-networks-david-duvenaud/))

**Definition.** Given $(\Omega, \mathscr{A}, P)$, a <span style="color:red">*random variable*</span> is a <span style="color:blue">(measurable) function</span> 

$$X: \Omega \rightarrow \mathbb{R}$$

s.t. $[X \le x] := \\{\omega \in \Omega : X(\omega) \le x\\} \in \mathscr{A}$ for all $x \in \mathbb{R}$.

**Remark.**
1. "measurable function"
2. Notation: $X, Y$ random variables, $x, y$ values

**Definition.** The <span style="color:red">*CDF of a random variable $X$*</span> is the function 

$$F = F_X : \mathbb{R} \rightarrow [0,1]$$

given by $F(x) = P[X \le x]$.

**Exercise.** Prove that $F_X$ is indeed a CDF.

**Definition.** The distribution of $X$ is the probability measure $P_X$ on $\mathbb{R}$ s.t.

$$P_X(A) = P[X \in A]$$

for all $A \in \mathbb{B}(\mathbb{R})$.

**Exercise.** $P_X$ is a Borel probability measure.

**Claim.** $P_X$ is the probability measure induced by $F$.

Proof: If we denote by $Q$ the probability measure corresponding to $F$, then

$$Q((-\infty, x]) = F(x) = P[X \le x] = P[X \in (-\infty, x]] = P_X((-\infty, x]).$$

Thus, $Q = P_X$.

---

#### (PP 3.2) Types of Random Variables

**Definition.** A RV $X$ is *discrete* if $X(\Omega) = \\{ X(\omega) : \omega \in \Omega\\}$ is countable.  
(i.e. $X(\Omega) = \\{x_1, x_2, \ldots \\}$)

**Definition.** A RV *$X$ has density $f$* if $F(x) = \int_{-\infty}^x f(\mu)d\mu$ for all $x \in \mathbb{R}$  
(for some integrable $f: \mathbb{R} \rightarrow [0, \infty]$).

---

#### (PP 3.3) Discrete Randome Variables

**Definition.** The probability mass function (PMF) of a discrete random variable $X$ is the function 

$$p: \mathbb{R} \rightarrow [0,1]$$

such that $p(x) = P[X = x]$.

**Remark.** 

If we set $S = X(\Omega)$, 
$$\sum_{x \in A \cap S} p(x)= P[X \in A] = P_X(A) = P_X(A \cap S) + P_X(A \cap S^c) = \sum_{x \in A \cap S} P[X = x].$$
In particular, $P(\mathbb{R}) = \sum_{x \in S} p(x) = 1$.

**Notation.** When we simply write $X \sim p$, we're assuming the underlying PMS $(\Omega, \mathscr{A}, P)$.
Also write $X \sim F$ and $X \sim Q$, which may casue confusion in some situations.

**Example.**
1. $X \sim$ Bernoulli$(\alpha)$, $\alpha \in [0,1] , p(1) = \alpha, p(0) = 1- \alpha$
2. $X \sim$ Binomial$(n,\alpha)$, $\alpha \in [0,1], p(k) = \left(\begin{matrix}
    n \\\
    k
  \end{matrix}\right)\alpha^k(1-\alpha)^{n-k}$ where $k \in \\{0, 1, \ldots \\}$, $\left(\begin{matrix}
    n \\\
    k
  \end{matrix}\right) = \frac{n!}{k!(n-k)!}$.
3. $X \sim$ Geometric$(\alpha)$, $\alpha \in [0,1], p(k)=(1-\alpha)^{k-1}\alpha$, $k \in \\{1, 2, \ldots\\}$.
4. $X \sim$ Possion$(\lambda)$,  $\lambda \ge 0, p(k) = e^{-\lambda}\frac{\lambda^k}{k!}$, $k \in \\{0, 1, \ldots, \\}$

---

#### (PP 3.4) Random Variables with Densities

---

#### (PP 5.1) Multiple discrete random variables

**Definition.** Given $(\Omega, \mathscr{A}, P)$, a <span style="color:red">*random vector*</span> is a measurable function 

$$X : \Omega \rightarrow \mathbb{R}^d$$

where $d \in \mathbb{N}$.

**Definition.** A *discrete random vector* $X \in \mathbb{R}^d$ is s.t. $X(\Omega)$ is countable.

**Definition.** The (joint) PMF (or joint distribution) of a discrete random vector $X \in \mathbb{R}^d$ is the function

$$p:\mathbb{R}^d \rightarrow [0,1]$$ 

such that $p(x)= P[X=x]$ for all $x \in \mathbb{R}^d$. 

**Notation.** $X = (X_1, \ldots, X_d)$, $x = (x_1, \ldots, x_d)$  
($X = x$ means $X_i = x_i$ for all $i$)  
$p(x) = p(x_1, \ldots, x_d)$  
$p_X(x) = p(x)$  

**Remark.** 

$$P[X \in A] = \sum_{x\in \mathscr{X}, x \in A} p(x)$$

where $\mathscr{X} = X(\Omega)$.

**Reamrk.** $g(X)$ is a random vector if $X \in \mathbb{R}^d$ is a random variable and
$g: \mathbb{R}^d \rightarrow \mathbb{R}^k$ is measurable.

**Proposition.**

$$\mathbb{E}(g(X)) = \sum_{x\in \mathscr{X}} g(x)p(x)$$

for any measurable $g: \mathbb{R}^d \rightarrow \mathbb{R}$ such that this sum is well-defined.

---

#### (PP 5.2) Marginals and conditionals

Deal with 2-dimensional case only.

Fix $(X, Y) \in \mathbb{R}^2$ with $p(x, y) = P[X=x, Y=y]$.

**Definition.** The *marginal PMF* of $X$ is $p_X(x) = P[X = x]$.

**Proposition.**

$$p_X(x) = \sum_{y \in Y} p(x, y)$$

Proof. $p_X(x) = P[X=x] = \sum_{y \in \mathscr{Y}} P[X=x, Y=y] = \sum_{y \in \mathscr{Y}}p(x,y)$  
noting $\\{\omega \in \Omega : X(\omega) = x\\} 
= \cup_{y \in \mathscr{Y}}\\{\omega \in \Omega : X(\omega) = x, Y(\omega) = y\\}$ 

**Notation.** $p(x) = p_X(x)$, $p(y) = p_Y(y)$  

**Definition.** The *conditional PMF* of $X$ given $Y=y$ is 

$$p(x\vert y) = P[X=x \vert Y = y]$$

(when $P[Y=y] > 0$.)

**Remark.** 

$$p(x\vert y) = \frac{P[X=x, Y=y]}{P[Y=y]} = frac{p(x, y)}{p(y)}$$

**Definition.** The conditional expectation of $X$ given $Y=y$ (when $p(y) > 0$) is 

$$\mathbb{E}(X\vert Y=y) = \sum_{x \in \mathscr{X}} xp(x \vert y)$$

when this sum is well-defined.

**Remark.** $\mathbb{E}(X\vert Y)$ is a random variable that depends on the random variable $Y$.

---

$$ $$

*To be added..*

---

