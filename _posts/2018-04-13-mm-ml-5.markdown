---
layout: post
title:  "Notes on Machine Learning 5: MLE for Exponential Families"
date:   2018-04-13 00:00:00
author: 장승환
categories: Notes
tags: ML
---

#### (ML 5.1) (ML 5.2) Exponential families 

**Definition.** Let $\Theta \subset \mathbb{R}^k$. An *exponential family* is a set $\\{p_\theta : \theta \in \Theta\\}$ of MPFs or MDFs on $\mathbb{R}$ s.t.

$$p_\theta = \exp\left( \sum_{i=1}^m \eta_i(\theta) s_i(\theta)  \right) h(x)/z(x).$$

* $\theta \in \mathbb{R}^k$: parameter
* $x \in \mathbb{R}^d$: "distritution variable"
* $\eta_i : \Theta \rightarrow \mathbb{R}$ 
* $s_i : \mathbb{R}^d \rightarrow \mathbb{R}$ ([sufficient statistics](https://slack-redir.net/link?url=https%3A%2F%2Fonlinecourses.science.psu.edu%2Fstat414%2Fnode%2F244)) 
* $h : \mathbb{R}^d \rightarrow [0, \infty)$ (support & scaling)
* $z : \Theta \rightarrow [0, \infty)$ (partition function: sort of normaizing constant but having very special properties)

$$p_\theta(x) = e^{\eta(\theta)^Ts(x)}h(x)/z(\theta)$$

($\eta = (\eta_1, \ldots, \eta_m)$, $s = (s_1, \ldots, s_m)$)

**Example (Exponential distribution).** (PDF)  
Exponential($\theta$), $\theta \in \Theta = [0, \infty)$ 

$$p_\theta(x) = \theta e^{-\theta x}I(x\ge 0)$$

* $\eta(\theta) = \theta$
* $s(x) = -x$
* $h(x) = I(x\ge 0)$
* $z(\theta) = 1/\theta$ 
* $k = d= 1$

**Example (Bernoulli distribution).** (PMF)  
Bernoulli($\theta$), $\theta \in \Theta = (0, 1)$

$$\begin{aligned}
p_\theta &=
\begin{cases}
\theta, \, {\rm if} \,\,\, x=1 \\
1-\theta, \, {\rm if} \,\,\, x=0 \\
0, \, {\rm else}
\end{cases} \\
&= \theta^{I(x=1)(1-\theta)^{I(x=0)}h(x)} \\
&= h(x)\exp \log \left( \theta^{I(x=1)}(1-\theta)^{I(x=0)}h(x) \right) \\
&= h(x)\exp \left( I(x=1)\log \theta + I(x=0)\log(1-\theta) \right) \\
\end{aligned}$$

* $\eta_1(\theta) = \log \theta$
* $\eta_2(\theta) = \log (1-\theta)$
* $s_1(x) = I(x=1)$
* $s_2(x) = I(x=0)$
* $h(x) = I_{\\{0, 1\\}}(x) = I(x \in \\{0,1\\})$

Not necessarily unique representation! :

$$\begin{aligned}
p_\theta &= \theta^{I(x=1)}(1-\theta)^{1-I(x=1)}h(x) \\
&= (1-\theta)\left(\frac{\theta}{1-\theta}\right)^{I(x=1)}h(x) \\
&= h(x)(1-\theta) \exp\left(I(x=1)\log \left(\frac{\theta}{1-\theta}\right) \right)
\end{aligned}$$

* $\eta(\theta) = \log \left(\frac{\theta}{1-\theta}\right)$
* s(x) = I(x=1)
* $z(\theta) \frac{1}{1-\theta}$

*Natural/Canonical form*  
* $\eta(\theta) = \theta$ $\,$ ($m= k$) 
* $\eta_1(\theta) = \theta_1, \ldots, \eta_k(\theta) = \theta_k$

**Example.**
1. (PDF) Exponential, Normal, Beta, Gamma, Chi-square
2. (PMF) Bernoulli, Binomial, Poisson, Geometric, Multinomial

Properties of exponential families:
* Conjugate prior
* Maximum entropy

**Example (NOT an exponential )family.**  
* Uniform$(0, \theta)$

cf. Uniform$(0,a) = \frac{1}{a}I(x \in (0,a))$, for a fixed $a \in (0, \infty)$, gives an exponential family (trivially).

---

#### (ML 5.3) (ML 5.4) MLE for an exponential family

$p_\theta(x) = e^ h(x)/z(\theta)$

Given data $D = (x_1, \ldots, x_n)$ with $x_i \in \mathbb{R}^d$ gotten via $X_1, \ldots, X_n \sim p_\theta$ iid.

Wanna compute $\theta_{\rm MLE} = \arg\max_{\theta \in \Theta} p(D \vert \theta)$.

$$\begin{aligned}
p(D\vert \theta) &= \prod_{i=1}^n p(x_i \vert \theta) \\
&= \prod_{i=1}^n e^{\theta^T s(x_i)}h(x_i)/z(\theta) \\
&= z(\theta)^{-n} e^{\theta^T \sum_{i=1}^ns(x_i)} \\
&= z(\theta)^{-n} e^{\theta^T s(D)} \prod_{i=1}^n h(x_i) 
\end{aligned}$$

$\log p(D \vert \theta) = -n\log z(\theta) + \theta^T s(D) + \sum_{i=1}^n \log h(x_i)$

$0 = \frac{\partial}{\partial \theta_j} \log p(D|vert\theta) 
= -n\frac{\partial}{\partial \theta_j} \log z(\theta) + s_j(D)$

where $\theta^Ts(D = \sum_{j=1}^k \theta_j s_j(D)$, $s_j(D) = \sum_{i=1}^n s_j(x_i)$ and $s=(s_i,\ldots, s_k)$.

Note $z(\theta) = \int_{\mathbb{R}^d} e^{\theta^T s(x)}h(x)dx$.

Then 

$$\begin{aligned}
\frac{\partial}{\partial \theta_j} \log z(\theta) &= \frac{1}{z(\theta)} \frac{\partial}{\partial \theta_j} z(\theta) \\
&= \frac{1}{z(\theta)} \int s_j(x)e^{\theta^Ts(x)}h(x)dx \\
&= \int s_j(x) p_\theta(x)dx = E_\theta s_j(X)
\end{aligned}$$

<figure>
<img src="/assets/pics/mm-ml/mle-exp-fam.png" alt="MLE for an exponential family" style="width: 50%; height: 50%">
<figcaption>MLE for an exponential family
</figcaption>
</figure>

$\log p(D \vert \theta) = -n\log z(\theta) + \theta^T s(D) + \sum_{i=1}^n \log h(x_i) = -nE_\theta s_j(x) +s_j(D)$

implies

$n E_\theta s(x) = s(D) = \sum_{i=1}^n s(x_i)$ 

implies 

<span style="color:red">$E_{\theta_{\rm MLE}} s(x) = \frac{1}{n}\sum_{i=1}^n s(x_i)$</span>
if MLE exists and $\theta_{\rm MLE} \in {\rm Int} \Theta$.

---

#### (ML 6.1) Maximum a posteriori (MAP) estimation

**Setup.**
* Given data $D = (x_1, \ldots, x_n)$, $x_i \in \mathbb{R}^d$.
* Assume a jont distribution $p(D, \theta) = p(D\vert \theta)p(\theta)$ where $\theta$ is a RV.
* Goal: choose a good value of $\theta$ for $D$.
* Choose <span style="color:red">$\theta_{\rm MAP} = \arg\max_\theta p(\theta\vert D)$</span>. $\,$ 
cf. <span style="color:blue">$\theta_{\rm MLE} = \arg\max_\theta p(\theta\vert D)$</span>.

<figure>
<img src="/assets/pics/mm-ml/likelihood-prior-posterior.png" alt="likelihood-prior-posterior" style="width: 50%; height: 50%">
<figcaption>Comparison among likelihood, prior and posterior
</figcaption>
</figure>

**Pros.**
* Easy to compute & interpretable
* Avoid overfitting, closely connected with "*regularization*"/"*shrinkage*"
* Tends to look like MLE asymptotically ($n \rightarrow \infty$)

**Cons.**
* Point estimate - no representation of uncertainty in $\theta$
* Not invariant under reparametrization (cf. $\mathscr{T} = g(\theta) \Rightarrow \mathscr{T}_{\rm MLE} = g(\mathscr{T}_{\rm MLE})$)
* Must assume prior on $\theta$

---

#### (ML 7.1) Bayesian inference - A simple example

"Put distributions on everything, and then use rules of probability."

---

#### (ML 7.2) Aspects of Bayesian inference


---

#### (ML 11.1) Estimators

Assume the data $D = (X_1, \ldots, X_n)$ are given as RVs.

**Definition.** A *statistic* is a RV $S$ that is a function of the data $D$. (i.e. $S=f(D)$)

**Terminology.** An *estimator* is a statistic intended to approximate a parameter governing the distribution of $D$.

**Notation.** 
1. $\hat{\theta}$ denotes an estimator of a parameter $\theta$.
2. $\hat{\theta}_n$ emphasize (the dependence on) $n$

**Example.** $X_1, \ldots, X_n \sim N(\mu, \sigma^2)$ iid  
(Sample mean) $\,\,$  $\hat{\mu} = \bar{X} = \frac{1}{n}\sum_{i=1}^nX_i$ $\,\,$ /cf. $\sigma^2 = E((X - \mu)^2)$  
("Biased" sample variance) $\,\,$  $\hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^n(X_i -\bar{X})^2$  
("unbiased" sample variance) $\,\,$  $s^2 = \frac{1}{n-1}\sum_{i=1}^n(X_i -\bar{X})^2$

**Definition.**
1. The *bias* of an estimator $\hat{\theta}$ is $\,$ ${\rm bias}(\hat{\theta}) = E(\hat{\theta}) - \theta$.  
2. An estimator $\hat{\theta}$ is *unbiased* if $\,$ ${\rm bias}(\theta) = 0$.  

**Example.**
1. $\hat{\mu}$ is unbiased: $E(\hat{\mu}) = E(\frac{1}{n}\sum_{i=1}^nX_i) =\frac{1}{n}\sum E(X_i) = \frac{1}{n}\sum \mu = \mu$  
2. $\hat{\sigma}^2$ is biased. (Exercise)
3. $s^2$ is unbiased. (Exercise)

---

#### (ML 11.2) Decision theory terminology in different contexts

**General**             | **Estimators**                         | **$^*$Regression/Classification**
Decision rule $\delta$  | $^*$Estimator function $g$             | Prediction function $f$
State $s$ (unknown)     | Parameter $\theta$ (unknown)           | Target value $Y$ (unknown)
$^*$Data $D$ (observed) | Data $D$ (observed)                    | Point $X$ (observed)
Action $a = \delta(D)$  | Estimator/Estimate $\hat{\theta}=g(D)$ | Prediction $\hat{Y} = f(X)$
Loss $L(s, a)$          | Loss L(\theta, \hat{\theta})           | Loss $L(Y, \hat{Y})$

**Example**  
Estimator is a RV: $\hat{\mu} = \frac{1}{n} \sum_{i=1}^n X_i$  
Estimate is a number: $\hat{\mu} = \frac{1}{n} \sum_{i=1}^n x_i = 2.3$  
SIn some situation the procedure $g$ is refered to as an estimator!

---

#### (ML 11.3) Frequentist risk, Bayesian expected loss, and Bayes risk

Exciting session to clear up all the mud!

**Data:** $\,$ $D = (X_1, \ldots, X_n)$, $D \sim p_\theta$  
**Parameter:** $\,$ $\theta \sim \pi$ $\,$ (the parameter $\theta$ is a RV)  
**Estimator:** $\,$ $\hat{\theta} = f(D) = \delta(D)$  

Everything begins with : Loss $=L(\theta, f(D))$.  
We wanna minimize the loss but it's an RV!  
Two option to deal with it:  
1. Averaging over $\theta$ given the data : $E(L(\theta, f(D)) \vert D) =:\rho(\pi, f(D))$ <span style="color:blue">Bayesian expected loss</span> 
2. Averaging over the data given $\theta$ : $E(L(\theta, f(D)) \vert \theta) =: R(\theta, f)$ <span style="color:red">(Frequentist) risk</span>


<figure>
<img src="/assets/pics/mm-ml/bayes-risk.png" alt="Bayes Risk" style="width: 100%; height: 100%">
<figcaption>
</figcaption>
</figure>


[Bayesians vs. frequentists](http://planning.cs.uiuc.edu/node471.html){:target="_blank"}

---

#### (ML 11.4) Choosing a decision rule - Bayesian and frequentist

<span style="color:orange">**How to choose $f$.**</span>  

<span style="color:blue">**Bayesian:** Assume $\pi$</span>  
<span style="color:blue">Case 1. Know $D$. Choose $f(D)$ to minimize $\rho(\pi, f(D))$</span>    
<span style="color:blue">Case 2. Don't know $D$. Choose $f$ to minimize $r(\pi, f)$</span>  

<span style="color:red">**Frequentist:** Introduce a furthere principle to guide your choice.</span>  
<span style="color:red">(a) Unbiasedness</span>  
<span style="color:red">(b) Admissibility</span> 
<span style="color:red">(c) Minimax</span>  
<span style="color:red">(d) Invariance</span>  

<figure>
<img src="/assets/pics/mm-ml/frequentist.png" alt="A frequentist approaches" style="width: 70%; height: 70%">
<figcaption>A frequentist approach
</figcaption>
</figure>

---

#### (ML 11.5) Bias-Variance decomposition (MSE $=$ bias$^2$ + var)

"A super impportant port of ML is what's called <span style="color:red">model selection</span> and a tool for model selection is the bias-variance decomposition."

Almost trivial identity but extremely handy.

**Definition.** Let $D$ be random data. The MSE of an estimator $\hat{\theta} = f(D)$ for $\theta$ is

$${\rm MSE}(\hat{\theta}) = E((\hat{\theta} - \theta)^2\vert \theta)$$

Put $\vert \theta$ emphasizing we're not averagning over $\theta$ here  (we don't have a distribution over $\theta$).
We're just averaging over the data. 

MSE$\theta$ is nothing but the risk $R(\theta, f)$ under square loss, i.e., when the loss function is the square of the deifference.

**Recall.** bias$(\hat{\theta}) = E(\theta) -\theta$.

**Proposition.** MSE$(\theta) = bias(\hat{\theta})^2 + {\rm var}(\hat{\theta})$

Proof:
<figure>
<img src="/assets/pics/mm-ml/proof-bias-variance.png" alt="Bias-variance" style="width: 70%; height: 70%">
<figcaption>
</figcaption>
</figure>

**Silly example.**
$X \sim N(\theta, 1)$
$\theta$ nonrandom & unknown
$D = X

"Natural" estimate of $\theta$: $\delta_1(D) = X \leadsto$ bias$^2 = 0$, var$ = 1$, MSE$ =1$  
"Silly" estimate of $\theta$: $\delta_0(D) = X \leadsto$ bias$^2 = \theta^2$, var$ = 0$, MSE$ = \theta^2$

cf. Shrinkage, Stein's paradox

---

#### (ML 12.1) Model selection - introduction and examples

<span style="color:orange">**"Model" selection**</span> really means "complexity" selection!

Here, *complexity* $\approx$ flexibility to fit/explain data  

**Example** (Linaer regression with MLE for $w$)  $f(x) = w^T\varphi(x)$  
Given data $x \in \mathbb{R}$, consider polynomial basis $\varphi(x) = x^k$, $\varphi = (\varphi_0, \varphi_1, \ldots, \varphi_B)$

Turns out <span style="color:purple">$B =$ "complexity parameter"</span>

<figure>
<img src="/assets/pics/mm-ml/model-selection.png" alt="Model selection illustration" style="width: 80%; height: 80%">
<figcaption>Model (in fact complexity) selection illustration
</figcaption>
</figure>

**Example** (Bayesia linear regression or MAP)
<figure>
<img src="/assets/pics/mm-ml/bayesian-regression.png" alt="Bayesian linear regression" style="width: 100%; height: 100%">
<figcaption>
</figcaption>
</figure>

**Example** ($k$NN)   
<figure>
<img src="/assets/pics/mm-ml/knn-model-selection.png" alt="Classification example" style="width: 80%; height: 80%">
<figcaption>
</figcaption>
</figure>
<span style="color:purple">$k$</span> "controls" decesion boundaties.


---

#### (ML 12.2) Bias-variance in model selection

Bias-variance trade-off, as they say.  
MSE $=$ bias$^2 +$ var  / $\in$MSE $=$ $\int$bias$^2 +$ $\int$var (only applies for <span style="color:green">square loss</span>)
<figure>
<img src="/assets/pics/mm-ml/bias-var-convex.png" alt="Bias-variance trade-off" style="width: 60%; height: 60%">
<figcaption>Bias-variance "trade-off"
</figcaption>
</figure>

<figure>
<img src="/assets/pics/mm-ml/bias-var-trade.png" alt="Bias-variance trade-off" style="width: 80%; height: 80%">
<figcaption>
</figcaption>
</figure>

---

#### (ML 15.1) Newton's method (for optimization) - intuition

2nd order method!

(Gradient descent $x_{t+1} = x_t - \alpha_t \nabla f(x_t)$ : 1st order method)

**Analogy (1D).**

* zero-finding: $x_{t+1} = x_t - \frac{f(x_t)}{f'(x_t)}$

<figure>
<img src="/assets/pics/mm-ml/zero-finding.png" alt="zero-finding" style="width: 35%; height: 35%">
<figcaption>
</figcaption>
</figure>

* min./maximizing: $x_{t+1} = x_t - \frac{f'(x_t)}{f''(x_t)}$

<figure>
<img src="/assets/pics/mm-ml/minimizing.png" alt="minimizing" style="width: 60%; height: 60%">
<figcaption> Minimizing in 1D
</figcaption>
</figure>

<figure>
<img src="/assets/pics/mm-ml/2d.png" alt="minimizing in 2D" style="width: 60%; height: 60%">
<figcaption> Minimizing in 2D
</figcaption>
</figure>

---

#### (ML 15.2) Newton's method (for optimization) in multiple dimensions

Idea: "Make a 2nd order approximation and minimize tha."

Let $f: \mathbb{R}^n \rightarrow \mathbb{R}$ be (sufficiently) smooth.

**Taylor's theorem:** for $x$ near a, letting $g = \nabla f(a)$ and $H = \nabla^2f(a) = \left(\frac{\partial^2}{\partial x_i \partial x_j}f(a)\right)_{ij}$,

$$\begin{aligned}
f(x) &\approx f(a) + g^T(x-a) +\frac{1}{2}(x-a)^TH(x-a) \\
     &= \frac{1}{2}x^THx + b^Tx +c =:q(x)
\end{aligned}$$

<figure>
<img src="/assets/pics/mm-ml/newton.png" alt="Newton's method" style="width: 35%; height: 35%">
<figcaption> 
</figcaption>
</figure>

Minimize: 
$0 = \nabla q = Hx + b \Rightarrow x = -H^{-1}b = a -H^{-1}g$

Critical to check: $\nabla^2 q = H$ $\Rightarrow$ minimum if $H$ is PSD.

**Algorithm.**
* Initialize $x \in \mathbb{R}^n$
* Iterate: $x_{t+1} = x_t - H^{-1}g$ where $g = \nabla f(x_t), H = \nabla^2 f(x_t)$

**Issues.**
1. $H$ may fail to be PSD. (Option: switch gradient descent. A smart way to do it: Levenberg–Marquardt algorithm) 
2. Rather than invert $H$, sove $Hy = g$ for $y$, then use $x_{t+1} = x_t - y$. (More robust approach)
3. $x_{t+1} = x_t - \alpha_t y$. (small "step size" $\alpha_t>0$)

---

#### (ML 17.1) Sampling methods - why sampling, pros and cons

Why sampling?
* For approximate expectations (estimate statistics / posterior infernce i.e. computing probability)
* For visualization

Why expectations?
* Any probability is an expectation: $P[X \in A] = E[I(X \in A)]$.
* Approximation is needed for intractable sums/integrals (can be expressed as expectations)

Pros.
* Easy (both to implement and understand)
* General purpose

Cons.
* Too easy - used inappropriately
* Slow
* Getting "good" samples may be dificult
* Difficult to assess

---

#### (ML 17.2) Monte Carlo methods - A little history

<figure>
<img src="/assets/pics/mm-ml/mc-history.png" alt="A little history of MC" style="width: 80%; height: 80%">
<figcaption>A little history of Monte Carlo methods
</figcaption>
</figure>

---

#### (ML 17.3) Monte Carlo approximation

Goal: Aprroximate $E[f(X)]$, when intractable.

Definition (Monte Carlo estimator): If $X_1, \ldots, X_n \sim p$ iid then 

$$\hat{\mu}_n = \frac{1}{n}\sum_{i=1}^nf(X_i)$$

is a (basic) *Monte Carlo estimator* of $E[f(X)]$ where $X \sim p$. (sample mean)

Remark  
(1) $E[\hat{\mu}_n] = E[f(X)]$ (i.e. $\hat{\mu}_n$ is an unbiased estimator)  
(2) 

---

#### (ML 17.5) Importance sampling - introduction
<span style="color:red">It's not a sampling method but an estimation technique!</span>

It can be though of as a variant of MC estimation.

Recall: MC estimation (by sample mean): 

$$E[f(X)] \approx \frac{1}{n}\sum_{i=1}^nf(X_i)$$

under the BIG assumtion that $X \sim p$ and $X_i \sim p$.

Can we do something similar by drawing samples from an alternative distribution $q$?

Yes, and in some cases you can do much much better!

($p$ density case)

$$E[f(X)] = \int f(x)p(x)dx = \int f(x)\frac{p(x)}{q(x)}p(x)dx \approx \frac{1}{n}\sum_{i=1}^nf(X_i)\frac{p(X_i)}{q(X_i)}$$

holds for all (pdf) $q$ s.t. $q(x)= 0 \Rightarrow p(x) = 0$, i.e., $p$ is absolutely continuous w.r.t. $q$.

<figure>
<img src="/assets/pics/mm-ml/importance-sampling.png" alt="Importance sampling" style="width: 80%; height: 80%">
<figcaption>Importance sampling
</figcaption>
</figure>

---

$$ $$

*To be added..*

---


