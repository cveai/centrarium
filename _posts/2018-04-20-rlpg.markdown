---
layout: post
title:  "RLPG discussions"
date:   2018-04-20 00:00:00
author: 장승환
categories: 기계학습
tags: 강화학습 RL 
---

 RLPG 그룹 멤버들간의 토론을 통해 이 노트가 만들어지고 있습니다:

[권휘](https://whikwon.github.io/){:target="_blank"} 김경환(부랩짱, 윈짱) 김민지(맥짱) 류주영 박창규 백병인 이규복 [이승재](https://github.com/seungjaeryanlee){:target="_blank"} 전효정 조동헌(우짱) [이일구](https://github.com/ilguyi?tab=repositories){:target="_blank"}(코짱) 정재윤 최윤규

---

#### Bellman equation

 Bellman equation 은 MDP 형태로 주어진 강화학습 문제에서 임의의 policy $\pi$ 가 주어졌을 때, 이에 해당하는 value 함수 
 $v_\pi: \mathscr{S} \rightarrow \mathbb{R}$ 와 
 $q_\pi: \mathscr{S} \times \mathscr{A} \rightarrow \mathbb{R}$
 가 각각 만족하게 되는 방정식이다.

먼저 state value 함수 $v_\pi$ 의 경우:

$$\begin{aligned}
v_\pi(s) &= \mathbb{E}(G_t \vert S_t = s) \\
&= \mathbb{E}(R_{t+1} + \gamma \,G_{t+1} \vert S_t = s) \\
&= \mathbb{E}(R_{t+1} \vert S_t = s) + \gamma \,\mathbb{E}_{S_t = s}(G_{t+1}\vert S_{t+1}) \\
&= \mathbb{E}(R_{t+1} \vert S_t = s) + \gamma \,\mathbb{E}_{S_t = s} \, v_\pi(S_{t+1})
\end{aligned}$$

action-value 함수 $q_\pi(s, a)$ 의 경우도 마찬가지로:

$$\begin{aligned}
q_\pi(s, a) &= \mathbb{E}(G_t \vert S_t = s, A_t = a) \\
&= \mathbb{E}(R_{t+1} + \gamma \,G_{t+1} \vert S_t = s, A_t = a) \\
&= \mathbb{E}(R_{t+1} \vert S_t = s, A_t = a) + \gamma \,\mathbb{E}_{S_t = s, A_t = a}(G_{t+1}\vert S_{t+1}, A_{t+1}) \\
&= \mathbb{E}(R_{t+1} \vert S_t = s, A_t = a) + \gamma \,\mathbb{E}_{S_t = s, A_t = a} \, q_\pi(S_{t+1}, A_{t+1})
\end{aligned}$$

State value 함수의 Bellman equation 을 이용하여 optimal policy $\pi^\star$ 를 정의할 수 있고 $\pi^\star$ 애 대한 value 함수 (optimal value 함수)
$v_\pi^\star$ 와 $q_\pi^\star$ 를 정의할 수 있는데 이 optimal value 함수들도 같은 형태의 Bellman equation 을 만족하게 된다. 
따라서 transition probability 가 알려져있는 경우 DP (Dynamic Programming) 을 이용하여 optimal policy $\pi^\star$ 로 수렴하는 
policy 의 수열을 생성하는 효율적인 알고리즘을 디자인할 수 있다.

 DP 를 이용한 알고리즘의 수렴성을 이용하여 transition probability 를 이용하지 않고 "optimal policy 로 수렴하는 알고리즘"을 디자인할 수 있는데
 sampling 을 이용한 통계학적인 접근이 그것이다. Sampling 을 이용한 방법은 bootstrapping 여부에 따라 Monte Carlo 와 Temporal Difference 두 가지가 있다.   

#### $Q$-learning as an off-policy TD learning

Off-policy control 방법은 behavior policy $\beta$ 와 tatget policy $\pi$ 를 구분하여 운용한다. 
학습을 위한 experience 를 획득하기 위한 action의 선택은 $\beta$ 를 통하여 한다. 
그리고 학습된 내용으로 policy $\pi$ 를 업데이트 하여 optimal policy 로 수렴되도록 한다.

그러면 target policy 를 업데이트 한다는 것은 어떤 의미인가? 본질적으로 optimal policy $\pi^\star$ 로 수렴시킬
목적으로 정의된 "target variable" $Q(s, a)$ 를 각 $(s, a)$ 마다 업데이트 하는 것을 의미한다.
즉, $Q \rightarrow q^\star$ 의 수렴성을 얻기 위한 업데이트이다.

Policy 는 보통 주어진 state 에서 가능한 action 들에 대한 확률함수로 주어지지만, 여기서는 편의상 behavoir policy 를 다음과 같은 형태의 함수로 정의하자.

$$\beta : \mathscr{S} \rightarrow \mathscr{A}.$$

즉, 주어진 state $s$ 에 대한 함숫값 $\beta(s) \in \mathscr{A}$ 는 $s$ 에서 policy $\beta$ 가 선택하는 action 이 된다.
다시한번 강조하자면 off-policy control 은 behavior policy $\beta$ 의 선택을 통하여 진행된 경험으로 얻은 reward 를 바탕으로 target policy $\pi$ 의 action value 함수에 해당하는 target variable $Q(s, a)$ 를 업데이트한다. 구체적으로 target policy 의 업데이트는 다음과 같은 형태로 이루어진다.

$$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha\left(R_{t+1} + \gamma \max_{a'} Q(S_{t+1},a') - Q(S_t, A_t)\right).$$

Behavior policy $\beta$ 에 의해 생성된 에피소드에서 timestep $t$ 에서의 state 가 $S_t = s$, 선택된 action 이 $A_t = a$ 라고 할 때 $Q(s, a)$ 의 값을 
좀 더 optimal 에 가까운 값으로 업데이트 하려는 의도인데, 그 방법이 현재의 값과 실제로 얻을 수 있는 값과의 차이를 계산해서 이를 반영하게 다는 것이다.
즉 현시점에서의 (tempoal) 차이 (difference) 를 근거로 삼아 업데이트 한다는 전략이다. 

#### Algoritm: Deep Q-learning with experience reply



#### 참고자료

[1] R. Sutton, A. Barto, *Reinforcement leaning: an introduction*, second edition ([final draft](http://incompleteideas.net/book/the-book-2nd.html){:target="_blank"}).  
[2] DQN   

---

*읽으시다 오류나 부정확한 내용을 발견하시면 꼭 알려주시길 부탁드립니다. 감사합니다.*  
