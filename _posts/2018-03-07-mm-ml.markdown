---
layout: post
title:  "Notes on Machine Learning"
date:   2018-03-08 00:00:00
author: 장승환
categories: Notes
tags: ML
---

*In this page I summarize in a succinct and straighforward fashion what I learn from [Machine Learning](https://www.youtube.com/watch?v=yDLKJtOVx5c&list=PLD0F06AA0D2E8FFBA){:target="_blank"} course by Mathematical Monk, along with my own thoughts and related resources.*
*I will update this page frequently, like every other day, until it's complete.*

---

**Acronyms**
* MM: Mathematical Monk
* ML: Machine Learning
* SL: Supervised Learning
* UL: Unsupervised Learning

* MCTC: Markov Chain Monte Carlo

---

#### (ML 1.1) What is machine learning?

"Designing *algorithms* for inferring what is unknown from knowns."

MM considers ML as a subfield of statistics, with emphasis on algorithms.

Got to read an interesting article [Machine Learning vs. Statistics](https://svds.com/machine-learning-vs-statistics/){:target="_blank"}, thanks to [Whi Kwon](https://whikwon.github.io/){:target="_blank"}.

**Applications**
 * Spam (filtering out)
 * Handwriting (recognition)
 * Google streetview
 * Netflix (recommendation systems)
 * Navigation
 * Climate modelling

---

#### (ML 1.2) What is supervised learning?

 Classification of ML problems: Supervised vs. Unsupervised
 
**Supervised**: Given $(x^{(1)}, y^{(1)}), \ldots, (x^{(n)}, y^{(n)})$ choose a *function* $f(x) = y$.
 * Classification: $y^{(i)} \in \\{$finite set$\\}$
 * Regression: $y^{(i)} \in \mathbb{R}$ or $y^{(i)} \in \mathbb{R}^d$

<figure>
<img src="/assets/pics/mm-ml/classification.png" alt="Classification" style="width: 30%; height: 30%">
<figcaption>Classification
</figcaption>
</figure>

<figure>
<img src="/assets/pics/mm-ml/regression.png" alt="Regression" style="width: 70%; height: 70%">
<figcaption>Regression
</figcaption>
</figure>

* $x^{(i)}$ : data point
* $y^{(i)}$ : class/value/label

---

#### (ML 1.3) What is unsupervised learning?

Much less well-defined.

**Unsupervised**: Given $x^{(1)}, \ldots, x^{(n)}$, find *patterns* in the data.
* Clustering (typical UL)
* Density estimation (much more well-defined)
* Dimensionality reduction
* many more

<figure>
<img src="/assets/pics/mm-ml/clustering.png" alt="Clustering" style="width: 40%; height: 40%">
<figcaption>Clustering
</figcaption>
</figure>

<figure>
<img src="/assets/pics/mm-ml/de-dr.png" alt="Density estimation and Dimensinality reduction" style="width: 80%; height: 80%">
<figcaption>Density estimation and Dimensinality reduction
</figcaption>
</figure>

---

#### (ML 1.4) Variations on supervised and unsupervised
* Semi-supervised
* Active Learning
* Decision theory
* Reinforcement Learning
 
---

$\cdots$

---

#### (ML 3.5) The Big Picture (part 1)

**Problem:** Minimize 

$${\rm EL}(Y, f(X))$$

using the obsrved $p(y\vert x)$ as the key quantity.

* EL: Expected Loss
* $y$: true value
* $f(x)$: our prediction

Core concepts and methods in ML fall out naturally from trying to solve this problem.

Data: $D = ((x_1, y_), \ldots, (x_n, y_n))$

**Discriminative** Estimate $p(y\vert x)$ directly using $D$.
* $k$NN
* Trees
* SVM

**Generative** Estimate $p(y, x)$ using $D$, and then recover $p(y\vert x) = \frac{p(x, y)}{p(x)}$.

**Parameters / Latent variables** $\theta$, consider $p_\theta(x, y) = p(x, y \vert \theta)$

$$p(y\vert x, D) = \int p(y\vert x, \theta, x, D) p(\theta\vert x, D) d\theta$$

<figure>
<img src="/assets/pics/mm-ml/big-picture.png" alt="Big picture" style="width: 80%; height: 80%">
<figcaption>Big picture
</figcaption>
</figure>


---

#### (ML 3.6) The Big Picture (part 2)
1. Exact inference (usually not possible)
* Multivariate Gaussian (very nice), Conjugate priors, Graphical models (us DP)
2. Point estimate of $\theta$ (simplest)
* MLE, MAP (Maximum A Posteriori)  
* Optimization, EM (Expectation Maximization) / Empirical Bayes
3. Deterministic Approximation
* Laplace Approximation, Variational methods, Expextation Propagation
4. Stochastic Approximation
* MCTC (Gibbs sampling, MH), Importance Sampling (Particle filter)

---

#### (ML 3.7) The Big Picture (part 3)

**Density estimation** (Unsupervised)

$D = (X_1, \ldots, X_n), X_i \in \mathbb{R}^d$, iid.

Goal: Estimate the distribution.

---

#### (ML 4.1) Maximum Likelihood Estimation (MLE) (part 1)

Setup: Given data $D = (x_1, \ldots, x_n), x_i \in \mathbb{R}^d$.
 
Assume a set distributions $\\{p_\theta : \theta \in Theta \\}$ on $\mathbb{R}^d$. ()

Assume $D$ is a sample from $X_1, \ldots, X_n ~ p_\theta$, iid for some $\theta \in \Theta$.

Goal: Estimate the true $\theta$ that $D$ comes from.

**Definition.** $\theta_{\rm MLE}$ for $\theta$ if $\theta_{\rm MLE} = \arg \max_{\theta \in \Theta} p(D\vert \theta).$

(more precisely, $p(D \vert \theta_{\rm MLE}) = \max_{\theta \in \Theta}p(D \vert \theta)$)

$p(D \vert \theta) := p(x_1, \ldots, x_n \vert \theta) = \prod_{i = 1}^n p(x_i \vert \theta) = \prod_{i = 1}^n P[X_i = x_i \vert \theta]$

**Remark**  
(1) MLE might not be unique.  
(2) MLE may fail to exist.  

---

#### (ML 17.5) Importance sampling - introduction
<span style="color:red">It's not a sampling method but an estimation technique!</span>

---

$$ $$

*To be added..*

---


