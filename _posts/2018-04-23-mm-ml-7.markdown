---
layout: post
title:  "Notes on Machine Learning 7: Bayesian Inference"
date:   2018-04-23 00:00:00
author: 장승환
categories: Notes
tags: ML
---

#### (ML 7.1) Bayesian inference - A simple example

<figure>
<img src="/assets/pics/mm-ml/bayes.gif" alt="Thomas Bayes" style="width: 35%; height: 35%">
<figcaption>Thomas Bayes
</figcaption>
</figure>

<span style="color:blue">"*Put distributions on everything, and then use rules of probability!*"</span>

<figure>
<img src="/assets/pics/mm-ml/bayes-rule.jpg" alt="Bayes' rule" style="width: 50%; height: 50%">
<figcaption>Bayes' rule
</figcaption>
</figure>

**Exampl.**
$D= (x_1, x_2, x_2) = (101, 100.5, 101.5)$ ($n=3$)   
$X \sim N(\theta, 1)$ iid given $\theta$  

$\theta_{\rm MLE} = \overline{x} = \frac{1}{n}\sum_{i=1}^n = 101$  

$ $

$\theta = N(100,1)$ prior  
$\theta_{\rm MAP} = 100.75$  
To compute $P(\theta < 100 \vert D)$, one needs *posterior* $p(\theta \vert D)$  

<span style="color:blue">posterior:</span>  
$p(\theta \vert D) = \frac{p(D\vert \theta)p(\theta)}{p(D)} = \frac{\prod_{i=1}^np(x_i \vert \theta) p(\theta)}{p(D)}$   
$\leadsto$ can be desribed as a normal distribution and so analytically computable!

<span style="color:orange">predictive:</span>  
$p(x \vert D) = \int p(x, \theta \vert D) d \theta = \int p(x\vert\theta)p(\theta \vert D) d \theta$  
$\leadsto$ analytically integrable!

---

#### (ML 7.2) Aspects of Bayesian inference

* <span style="color:blue">Bayesian inference</span> : Assume a prior distribution $p(\theta)$ and then use probability rules to work with $p(x\vert \theta)$ to anser questions.  
* <span style="color:blue">Bayesian procedures</span> : Minimize expected loss (averaging over $\theta$).  

* <span style="color:blue">Objective Bayesian</span> : use belief-based priors  
* <span style="color:blue">Subjective Bayesian</span> : use non-informative priors  

**Pros.** 
* Directly answer certain questions, e.g., can compute $P(99 < \theta < 101)$  
* Avoid some pathologies (associated with frequentist approach)  
* Avoid overfitting  
* Automatically do medel selection ("Occam's razor")  
* Bayesian procedures are often admissible  

**Cons.**  
* Must assume a prior  
* Exact computation (of posterior) can be intractable ($\leadsto$ have to use approximation)  

**Priors.**  
* Non-informative  
* Improper, e.g., $p(\theta) = 1$ with density  
* Conjugate  

---

#### (ML 7.3) Proportionality

"Using proportionality is a extraordinarily handy trick (big time-saver) when doing Bayesian inference."

**Notation.** $f \propto g$ if there exists $c \neq 0$ such that $g(x) = cf(x)$ for all $x$. 

**Claim.** If $f$ is a PDF and $f \propto g$ then $g$ uniquely determine $f$, and $f(x) = \frac{g(x)}{\int g(x)}dx$.

Proof. $\frac{g(x)}{\int g(x)}dx = \frac{cf(x)}{\int cf(x)}dx = f(x)$.

---

#### (ML 7.4) Conjugate priors

**Definition.** A family $\mathscr{F}$ of (prior) distributions $p(\theta)$ is 
<span style="color:red">*conjugate*</span> to a likelihood $p(D\vert \theta)$
if the posterior $p(\theta \vert D)$ is in $\mathscr{F}$.

**Examples.**
* Beta is conjugate to Bernoulli.
* Gaussian is conjugate to Gaussian (mean).
* Any exponential family has a conjugate prior.

---

#### (ML 7.5) (ML 7.6) Beta-Bernoulli model

For example, we model a sequence of binary outcomes, like coin flips, as Bernoulli random variables 
with Beta prior distribution on the probability of the heads.
In other words, Beta is a conjugate prior for Bernoulli.

Setup: $X_1, \ldots, X_n \sim {\rm Bern}(\theta)$ independen gieven $\theta$ with prior $\theta \sim {\rm Beta}(a,b)$.  
Here the parameters $a, b$ of the prior are called <span style="color:red">*hyperparameters*</span>.

For generaic $X \sim {\rm Bern}(\theta)$, we have

$$p(1\vert \theta) = P(X = 1 \vert \theta) = \theta = \theta^{I(X=1)}(1-\theta)^{I(X=0)}$$

where

$$\begin{aligned}
p(\theta) &= \frac{\theta^{a-1}(1-\theta)^{b-1}}{B(a,b)}\cdot I(\theta \in [0, 1]) \\
& \propto \theta^{a-1}(1-\theta)^{b-1}\cdot I(\theta \in [0, 1])
\end{aligned}$$

From the setup we generate data $D = (x_1, \ldots, x_n)$. 
Then it's easy to compute the posterior distribution: 

$$\begin{aligned}
p(\theta\vert D) & \propto_\theta p(D \vert \theta) p(\theta) \\
                 & = p(\theta)\prod_{i=1}p(X_i = x_i\vert \theta) \\
                 & \propto \theta^{a-1}(1-\theta)^{b-1}\theta^{\sum I(x_i=1)}(1-\theta)^{\sum I(x_i=0)} \\
                 & = \theta^{a-1}(1-\theta)^{b-1}\theta^{n_1}(1-\theta)^{n_0} \\
                 & = \theta^{a+n_1-1}(1-\theta)^{b + n_0-1} \\
                 & \propto \cdot {\rm Beta}(\theta\vert a+n_1, b+n_0)
\end{aligned}$$

where $n_1 := \sum I(x_i=1)$ and $n_0 := \sum I(x_i=0)$.

Thus, $p(\theta\vert D) = {\rm Beta}(\theta\vert a+n_1, b+n_0)$, and so Beta is conjugate to Bernoulli. 

<figure>
<img src="/assets/pics/mm-ml/beta-prior.png" alt="Beta distributions" style="width: 40%; height: 40%">
<figcaption>Beta distributions
</figcaption>
</figure>

See [Beta distribution - an introduction](https://youtu.be/v1uUgTcInQk){:target="_blank"} by [Ox educ](www.ox-educ.com){:target="_blank"} for intuition behind Beta prior.

If $\theta \sim {\rm Beta}(a, b)$, one has:
* $\mathbb{E}(\theta) = \frac{a}{a+b}$;
* $\sigma^2(\theta) = \frac{ab}{(a+b)^2(a+b+1)}$;
* mode $=\frac{a-1}{a+b-2}$.  
($a+n_1, b+n_0$ are called <span style="color:red">*pseudocounts*</span>)

For  $\theta \vert D \sim {\rm Beta}(\theta\vert a+n_1, b+n_0)$, on has:
* $\mathbb{E}(\theta\vert D) = \frac{a+n_1}{a+b+n}$;
* mode $=\frac{a+n_1-1}{a+b+n-2}$. 

Now let's make a few connections. We know that 
* $\theta_{\rm MLE} =$ empirical probability $= \frac{n_1}{n}$ /  $\left(\frac{n_0}{n}, \frac{n_1}{n}\right)$
* $\theta_{\rm MAP} = \frac{a+n_1-1}{a+b+n-2}$ 

posterior mean $=\frac{a+n_1}{a+b+n} = \frac{a+b}{a+b+n}\cdot \frac{a}{a+b} + \frac{n}{a+b+n}\cdot \frac{n_1}{n}$,  
i.e., a convex combination of prior mean & MLE

* When $n \rightarrow \infty$, posterior mean convergees to the MLE.
* When $n = 0$, if we have no data, we get back to the prior mean.

Let's compute now the <span style="color:red">(posterior) predictive distribution</span>:  

$$\begin{aligned}
P(X = 1\vert D) & = \int P(X = 1\vert)p(\theta \vert D) \\
                & = \int P(X = 1\vert \theta) p(\theta \vert D) \\
                & = \int \theta \cdot {\rm Beta}(\theta \vert a+n_1, b+n_0)d\theta \\
                & = \mathbb{E}(\theta \vert D) \\
                & = \frac{a+n_1}{a+b+n}. 
\end{aligned}$$        
        
---

#### (ML 7.7.A1) Dirichlet distribution

[Dirichlet](https://youtu.be/9f60JyTC6Oc){:target="_blank"}, one of the great mathematicians in 1800s.

Dirichlet distribution is a distribution on probability distributinos.

$\theta = (\theta_1, \ldots, \theta_n) \sim {\rm Dir}(\alpha)$ means:  

$$p(\theta) = \frac{1}{B(\alpha)}\prod_{i=1}^n \theta_i^{\alpha_i-1}I(\theta \in S)$$

where $\alpha = (\alpha_1, \ldots, \alpha_n)$, $\alpha_i >0$ are the parameters,
$S = \\{x \in \mathbb{R}^n: x_i \ge 0, \sum_{i=1}^n x_n =1 \\}$ is the probability simplex, and
$\frac{1}{B(\alpha)} =\frac{\Gamma(\alpha_0)}{\Gamma(\alpha_1)\cdots\Gamma(\alpha_n)}$.

<figure>
<img src="/assets/pics/mm-ml/probability-simplex.png" alt="Probability simplex" style="width: 40%; height: 40%">
<figcaption>Probability simplex
</figcaption>
</figure>

<figure>
<img src="/assets/pics/mm-ml/dirichlet.png" alt="Dirichlet distributions" style="width: 100%; height: 100%">
<figcaption>Dirichlet distributions
</figcaption>
</figure>

$E(\theta_i) = \frac{\alpha_i}{\alpha_0}$

${\rm mode} = \left(\frac{\alpha_1-1}{\alpha_0}, \ldots, \frac{\alpha_n-1}{\alpha_0-n}\\right)$

$\sigma^2(\theta) = \frac{\alpha_i(\alpha_0-\alpha_i)}{\alpha_0^2(\alpha_0+1)}$

---

(ML 7.9) Posterior distribution for univariate Gaussian (part 1)

Given dada $D = (x_1, \ldots, x_n)$ with $x_i \in \mathbb{R}$,  
want to know from which distribution $D$ might come fomm.  
Model $D$ as $X_1, \ldots, X_n \sim N(\mu, \sigma^2)$ for some $\mu, \sigma \in \mathbb{R}$,  
i.e., $N(\mu, \sigma^2)$ is the "true" distribution wa want.  
Becasue we do not know $\mu$, we model it as a random variable $\theta$, which itself follows a normal distribution:

$$\theta \sim N(\mu_0, \sigma_0^2).$$

We assume that $\sigma^2, \mu_0, \sigma_0^2$ are known to make the problem (more) tractable.


---

...

---

#### (ML 11.1) Estimators

Assume the data $D = (X_1, \ldots, X_n)$ are given as random variables.

**Definition.** A <span style="color:red">*statistic*</span> is a random variable $S = f(D)$ that is a function of the data $D$.

**Terminology.** An <span style="color:red">*estimator*</span> is a statistic intended to approximate a parameter governing the distribution of $D$.

**Notation.** 
1. $\hat{\theta}$ denotes an estimator of a parameter $\theta$.
2. $\hat{\theta}_n$ emphasize (the dependence on) $n$

**Example.** $X_1, \ldots, X_n \sim N(\mu, \sigma^2)$ iid  
(Sample mean) $\,\,$  $\hat{\mu} = \bar{X} = \frac{1}{n}\sum_{i=1}^nX_i$ $\,\,$ /cf. $\sigma^2 = \mathbb{E}((X - \mu)^2)$  
("Biased" sample variance) $\,\,$  $\hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^n(X_i -\bar{X})^2$  
("unbiased" sample variance) $\,\,$  $s^2 = \frac{1}{n-1}\sum_{i=1}^n(X_i -\bar{X})^2$

**Definition.**
1. The <span style="color:red">*bias*</span> of an estimator $\hat{\theta}$ is $\,$ ${\rm bias}(\hat{\theta}) = \mathbb{E}(\hat{\theta}) - \theta$.  
2. An estimator $\hat{\theta}$ is *unbiased* if $\,$ ${\rm bias}(\theta) = 0$.  

**Example.**
1. $\hat{\mu}$ is unbiased: $\mathbb{E}(\hat{\mu}) = \mathbb{E}(\frac{1}{n}\sum_{i=1}^nX_i) =\frac{1}{n}\sum \mathbb{E}(X_i) = \frac{1}{n}\sum \mu = \mu$  
2. $\hat{\sigma}^2$ is biased. (Exercise)
3. $s^2$ is unbiased. (Exercise)

---

#### (ML 11.2) Decision theory terminology in different contexts

**General**             | **Estimators**                         | **$^*$Regression/Classification**
Decision rule $\delta$  | $^*$Estimator function $g$             | Prediction function $f$
State $s$ (unknown)     | Parameter $\theta$ (unknown)           | Target value $Y$ (unknown)
$^*$Data $D$ (observed) | Data $D$ (observed)                    | Point $X$ (observed)
Action $a = \delta(D)$  | Estimator/Estimate $\hat{\theta}=g(D)$ | Prediction $\hat{Y} = f(X)$
Loss $L(s, a)$          | Loss $L(\theta, \hat{\theta})$           | Loss $L(Y, \hat{Y})$

**Example. (Estimators)**  
An estimator is a random rariable: $\hat{\mu} = \frac{1}{n} \sum_{i=1}^n X_i$.  
An estimate is a number: $\hat{\mu} = \frac{1}{n} \sum_{i=1}^n x_i = 2.3$.  
(In some situation, the procedure $g$ is refered to as an estimator!)

---

#### (ML 11.3) Frequentist risk, Bayesian expected loss, and Bayes risk

**Loss and Risk.** Exciting session to clear up all the mud!  

**Data:** $\,$ $D = (X_1, \ldots, X_n)$, $D \sim p_\theta$  
**Parameter:** $\,$ $\theta \sim \pi$ $\,$, i.e., the parameter $\theta$ is a random variable.
**Estimator:** $\,$ $\hat{\theta} = f(D) = \delta(D)$  

Everything begins with : $\,\,\,\,\,$ Loss $=L(\theta, f(D))$.  
We wanna minimize the loss but it's an RV!  
Two option to deal with it:  
1. Averaging over $\theta$ given the data : $E(L(\theta, f(D)) \vert D) =:\rho(\pi, f(D))$ <span style="color:blue">Bayesian expected loss</span> 
2. Averaging over the data given $\theta$ : $E(L(\theta, f(D)) \vert \theta) =: R(\theta, f)$ <span style="color:red">(Frequentist) risk</span>


<figure>
<img src="/assets/pics/mm-ml/bayes-risk.png" alt="Bayes Risk" style="width: 100%; height: 100%">
<figcaption>
</figcaption>
</figure>


[Bayesians vs. frequentists](http://planning.cs.uiuc.edu/node471.html){:target="_blank"}

---

#### (ML 11.4) Choosing a decision rule - Bayesian and frequentist

<span style="color:orange">**How to choose $f$.**</span>  

<span style="color:blue">**Bayesian:** Assume $\pi$</span>  
<span style="color:blue">Case 1. Know $D$. Choose $f(D)$ to minimize $\rho(\pi, f(D))$</span>    
<span style="color:blue">Case 2. Don't know $D$. Choose $f$ to minimize $r(\pi, f)$</span>  

<span style="color:red">**Frequentist:** Introduce a furthere principle to guide your choice.</span>  
<span style="color:red">(a) Unbiasedness</span>  
<span style="color:red">(b) Admissibility</span> 
<span style="color:red">(c) Minimax</span>  
<span style="color:red">(d) Invariance</span>  

<figure>
<img src="/assets/pics/mm-ml/frequentist.png" alt="A frequentist approaches" style="width: 70%; height: 70%">
<figcaption>A frequentist approach
</figcaption>
</figure>

---

#### (ML 11.5) Bias-Variance decomposition (MSE $=$ bias$^2$ + var)

"A super impportant port of ML is what's called <span style="color:red">model selection</span> and a tool for model selection is the bias-variance decomposition."

Almost trivial identity but extremely handy.

**Definition.** Let $D$ be random data. The MSE of an estimator $\hat{\theta} = f(D)$ for $\theta$ is

$${\rm MSE}(\hat{\theta}) = E((\hat{\theta} - \theta)^2\vert \theta)$$

Put $\vert \theta$ emphasizing we're not averagning over $\theta$ here  (we don't have a distribution over $\theta$).
We're just averaging over the data. 

MSE$\theta$ is nothing but the risk $R(\theta, f)$ under square loss, i.e., when the loss function is the square of the deifference.

**Recall.** bias$(\hat{\theta}) = E(\theta) -\theta$.

**Proposition.** MSE$(\theta) = bias(\hat{\theta})^2 + {\rm var}(\hat{\theta})$

Proof:
<figure>
<img src="/assets/pics/mm-ml/proof-bias-variance.png" alt="Bias-variance" style="width: 70%; height: 70%">
<figcaption>
</figcaption>
</figure>

**Silly example.**
$X \sim N(\theta, 1)$
$\theta$ nonrandom & unknown
$D = X

"Natural" estimate of $\theta$: $\delta_1(D) = X \leadsto$ bias$^2 = 0$, var$ = 1$, MSE$ =1$  
"Silly" estimate of $\theta$: $\delta_0(D) = X \leadsto$ bias$^2 = \theta^2$, var$ = 0$, MSE$ = \theta^2$

cf. Shrinkage, Stein's paradox

---

#### (ML 11.8) Bayesian decision theory

$$\rho(\pi, \delta(D)) = \mathbb{E}(L(\theta, \delta(D)) \vert D)$$  

$$\begin{aligned}
r(\pi, \delta) &= \mathbb{E}(L(\theta, \delta(D))) \\
&= \mathbb{E} (\mathbb{E}(L(\theta, \delta(D)) \vert D)) )
\end{aligned}$$

**Terminology.(Informal)**
* A *generalized Bayes rule* is a decision rule $\delta$ that minimizes $\rho(\pi, \delta(D))$ for each $D$.
* A *Bayes rule* minimizes is a decision rule $\delta$ that minimizes $r(\pi, \delta)$.

**Remark**
* GBR $\Rightarrow$ BR, but BR $\not\Rightarrow$ GBR.
* If $r(\pi, \delta) = \infty$ for all $\delta$, then anything is a BR,
but a GBR still makes sense.
* On a set of $\pi$-measure $0$, BR is arbitrary bit GBR is still sensible.

**Complete Class Theorems.** Under mild conditions:
1. Every GBR (for a proper $\pi$) is admissible;
2. Every admissible decision rule is a GBR for some (possibly improper) $\pi$

*Check out*:  
* [Complete Class Theorems for the Simplest Empirical Bayes Decision Problems](https://projecteuclid.org/euclid.aos/1176343749){:target="_blank"}
* [A Complete Class Theorem for Statistical Problems with Finite Sample Spaces](https://projecteuclid.org/euclid.aos/1176345645){:target="_blank"}

---

#### (ML 12.1) Model selection - introduction and examples

<span style="color:orange">**"Model" selection**</span> really means "complexity" selection!

Here, *complexity* $\approx$ flexibility to fit/explain data  

**Example** (Linaer regression with MLE for $w$)  $f(x) = w^T\varphi(x)$  
Given data $x \in \mathbb{R}$, consider polynomial basis $\varphi(x) = x^k$, $\varphi = (\varphi_0, \varphi_1, \ldots, \varphi_B)$

Turns out <span style="color:purple">$B =$ "complexity parameter"</span>

<figure>
<img src="/assets/pics/mm-ml/model-selection.png" alt="Model selection illustration" style="width: 80%; height: 80%">
<figcaption>Model (in fact complexity) selection illustration
</figcaption>
</figure>

**Example** (Bayesia linear regression or MAP)
<figure>
<img src="/assets/pics/mm-ml/bayesian-regression.png" alt="Bayesian linear regression" style="width: 100%; height: 100%">
<figcaption>
</figcaption>
</figure>

**Example** ($k$NN)   
<figure>
<img src="/assets/pics/mm-ml/knn-model-selection.png" alt="Classification example" style="width: 80%; height: 80%">
<figcaption>
</figcaption>
</figure>
<span style="color:purple">$k$</span> "controls" decesion boundaties.


---

#### (ML 12.2) Bias-variance in model selection

Bias-variance trade-off, as they say.  
MSE $=$ bias$^2 +$ var  / $\in$MSE $=$ $\int$bias$^2 +$ $\int$var (only applies for <span style="color:green">square loss</span>)
<figure>
<img src="/assets/pics/mm-ml/bias-var-convex.png" alt="Bias-variance trade-off" style="width: 60%; height: 60%">
<figcaption>Bias-variance "trade-off"
</figcaption>
</figure>

<figure>
<img src="/assets/pics/mm-ml/bias-var-trade.png" alt="Bias-variance trade-off" style="width: 80%; height: 80%">
<figcaption>
</figcaption>
</figure>

---

#### (ML 15.1) Newton's method (for optimization) - intuition

2nd order method!

(Gradient descent $x_{t+1} = x_t - \alpha_t \nabla f(x_t)$ : 1st order method)

**Analogy (1D).**

* zero-finding: $x_{t+1} = x_t - \frac{f(x_t)}{f'(x_t)}$

<figure>
<img src="/assets/pics/mm-ml/zero-finding.png" alt="zero-finding" style="width: 35%; height: 35%">
<figcaption>
</figcaption>
</figure>

* min./maximizing: $x_{t+1} = x_t - \frac{f'(x_t)}{f''(x_t)}$

<figure>
<img src="/assets/pics/mm-ml/minimizing.png" alt="minimizing" style="width: 60%; height: 60%">
<figcaption> Minimizing in 1D
</figcaption>
</figure>

<figure>
<img src="/assets/pics/mm-ml/2d.png" alt="minimizing in 2D" style="width: 60%; height: 60%">
<figcaption> Minimizing in 2D
</figcaption>
</figure>

---

#### (ML 15.2) Newton's method (for optimization) in multiple dimensions

Idea: "Make a 2nd order approximation and minimize tha."

Let $f: \mathbb{R}^n \rightarrow \mathbb{R}$ be (sufficiently) smooth.

**Taylor's theorem:** for $x$ near a, letting $g = \nabla f(a)$ and $H = \nabla^2f(a) = \left(\frac{\partial^2}{\partial x_i \partial x_j}f(a)\right)_{ij}$,

$$\begin{aligned}
f(x) &\approx f(a) + g^T(x-a) +\frac{1}{2}(x-a)^TH(x-a) \\
     &= \frac{1}{2}x^THx + b^Tx +c =:q(x)
\end{aligned}$$

<figure>
<img src="/assets/pics/mm-ml/newton.png" alt="Newton's method" style="width: 35%; height: 35%">
<figcaption> 
</figcaption>
</figure>

Minimize: 
$0 = \nabla q = Hx + b \Rightarrow x = -H^{-1}b = a -H^{-1}g$

Critical to check: $\nabla^2 q = H$ $\Rightarrow$ minimum if $H$ is PSD.

**Algorithm.**
* Initialize $x \in \mathbb{R}^n$
* Iterate: $x_{t+1} = x_t - H^{-1}g$ where $g = \nabla f(x_t), H = \nabla^2 f(x_t)$

**Issues.**
1. $H$ may fail to be PSD. (Option: switch gradient descent. A smart way to do it: Levenberg–Marquardt algorithm) 
2. Rather than invert $H$, sove $Hy = g$ for $y$, then use $x_{t+1} = x_t - y$. (More robust approach)
3. $x_{t+1} = x_t - \alpha_t y$. (small "step size" $\alpha_t>0$)

---

#### (ML 17.1) Sampling methods - why sampling, pros and cons

Why sampling?
* For approximate expectations (estimate statistics / posterior infernce i.e. computing probability)
* For visualization

Why expectations?
* Any probability is an expectation: $P[X \in A] = E[I(X \in A)]$.
* Approximation is needed for intractable sums/integrals (can be expressed as expectations)

Pros.
* Easy (both to implement and understand)
* General purpose

Cons.
* Too easy - used inappropriately
* Slow
* Getting "good" samples may be dificult
* Difficult to assess

---

#### (ML 17.2) Monte Carlo methods - A little history

<figure>
<img src="/assets/pics/mm-ml/mc-history.png" alt="A little history of MC" style="width: 80%; height: 80%">
<figcaption>A little history of Monte Carlo methods
</figcaption>
</figure>

---

#### (ML 17.3) Monte Carlo approximation

Goal: Aprroximate $E[f(X)]$, when intractable.

Definition (Monte Carlo estimator): If $X_1, \ldots, X_n \sim p$ iid then 

$$\hat{\mu}_n = \frac{1}{n}\sum_{i=1}^nf(X_i)$$

is a (basic) *Monte Carlo estimator* of $E[f(X)]$ where $X \sim p$. (sample mean)

Remark  
(1) $E[\hat{\mu}_n] = E[f(X)]$ (i.e. $\hat{\mu}_n$ is an unbiased estimator)  
(2) 

---

#### (ML 17.5) Importance sampling - introduction
<span style="color:red">*It's not a sampling method but an estimation technique!*</span>

It can be thought of as a variant of MC estimation.

**Recall.** MC estimation (by sample mean): 

$$\mathbb{E}(f(X)) \approx \frac{1}{n}\sum_{i=1}^nf(X_i)$$

under the BIG assumtion that $X \sim p$ and $X_i \sim p$.

Can we do something similar by drawing samples from an alternative distribution $q$?

*Yes*, and in some cases you can do much much better!

**Density $p$ case.**

$$\begin{aligned}
\mathbb{E}(f(X)) &= \int f(x)p(x)dx \\
&= \int f(x)\frac{p(x)}{q(x)}p(x)dx \\ 
&\approx \frac{1}{n}\sum_{i=1}^nf(X_i)\frac{p(X_i)}{q(X_i)}
\end{aligned}$$

holds for all pdf $q$ with respect to wchi $q$ is *absolutely continuous*, i.e., $p(x) = 0$ whenever $q(x)= 0$.

<figure>
<img src="/assets/pics/mm-ml/importance-sampling.png" alt="Importance sampling" style="width: 80%; height: 80%">
<figcaption>Importance sampling
</figcaption>
</figure>

---

#### (ML 17.6) Importance sampling - intuition

What makes a good/bad choice of the *proposal distrubution* $q$?



---

$$ $$

*To be added..*

---


