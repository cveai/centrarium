---
layout: post
title:  "RLPG discussions #1"
date:   2018-04-20 00:00:00
author: 장승환
categories: 기계학습
tags: 강화학습 RL 
---

 RLPG 그룹 멤버들간의 토론을 통해 이 노트가 만들어지고 있습니다:

[권휘](https://whikwon.github.io/){:target="_blank"} 김경환(부랩짱, 윈짱) 김민지(맥짱) 류주영 박창규 백병인 이규복 [이승재](https://github.com/seungjaeryanlee){:target="_blank"} 전효정 조동헌(우짱) [이일구](https://github.com/ilguyi?tab=repositories){:target="_blank"}(코짱) 정재윤 최윤규

---

### Bellman equation

 Bellman equation 은 MDP 형태로 주어진 강화학습 문제에서 임의의 policy $\pi$ 가 주어졌을 때, 이에 해당하는 value 함수 
 $v_\pi: \mathscr{S} \rightarrow \mathbb{R}$ 와 
 $q_\pi: \mathscr{S} \times \mathscr{A} \rightarrow \mathbb{R}$
 가 각각 만족하게 되는 방정식이다.

먼저 state value 함수 $v_\pi$ 의 경우:

$$\begin{aligned}
v_\pi(s) &= \mathbb{E}(G_t \vert S_t = s) \\
&= \mathbb{E}(R_{t+1} + \gamma \,G_{t+1} \vert S_t = s) \\
&= \mathbb{E}(R_{t+1} \vert S_t = s) + \gamma \,\mathbb{E}_{S_t = s}(G_{t+1}\vert S_{t+1}) \\
&= \mathbb{E}(R_{t+1} \vert S_t = s) + \gamma \,\mathbb{E}_{S_t = s} \, (v_\pi(S_{t+1}))
\end{aligned}$$

action-value 함수 $q_\pi(s, a)$ 의 경우도 마찬가지로:

$$\begin{aligned}
q_\pi(s, a) &= \mathbb{E}(G_t \vert S_t = s, A_t = a) \\
&= \mathbb{E}(R_{t+1} + \gamma \,G_{t+1} \vert S_t = s, A_t = a) \\
&= \mathbb{E}(R_{t+1} \vert S_t = s, A_t = a) + \gamma \,\mathbb{E}_{S_t = s, A_t = a}(G_{t+1}\vert S_{t+1}, A_{t+1}) \\
&= \mathbb{E}(R_{t+1} \vert S_t = s, A_t = a) + \gamma \,\mathbb{E}_{S_t = s, A_t = a} \, q_\pi(S_{t+1}, A_{t+1})
\end{aligned}$$

State value 함수의 Bellman equation 을 이용하여 optimal policy $\pi^\star$ 를 정의할 수 있고 $\pi^\star$ 애 대한 value 함수 (optimal value 함수)
$v_\pi^\star$ 와 $q_\pi^\star$ 를 정의할 수 있는데 이 optimal value 함수들도 같은 형태의 Bellman equation 을 만족하게 된다. 
따라서 transition probability 가 알려져있는 경우 DP (Dynamic Programming) 을 이용하여 optimal policy $\pi^\star$ 로 수렴하는 
policy 의 수열을 생성하는 효율적인 알고리즘을 디자인할 수 있다.

 DP 를 이용한 알고리즘의 수렴성을 이용하여 transition probability 를 이용하지 않고 "optimal policy 로 수렴하는 알고리즘"을 디자인할 수 있는데
 sampling 을 이용한 통계학적인 접근이 그것이다. Sampling 을 이용한 방법은 bootstrapping 여부에 따라 Monte Carlo 와 Temporal Difference 두 가지가 있다.   

---

### $Q$-learning as an off-policy TD learning

Off-policy control 방법은 behavior policy $\beta$ 와 tatget policy $\pi$ 를 구분하여 운용한다. 
학습을 위한 experience 를 획득하기 위한 action의 선택은 $\beta$ 를 통하여 한다. 
그리고 학습된 내용으로 policy $\pi$ 를 업데이트 하여 optimal policy 로 수렴되도록 한다.

그러면 target policy 를 업데이트 한다는 것은 어떤 의미인가? 본질적으로 optimal policy $\pi^\star$ 로 수렴시킬
목적으로 정의된 "target variable" $Q(s, a)$ 를 각 $(s, a)$ 마다 업데이트 하는 것을 의미한다.
즉, $Q \rightarrow q^\star$ 의 수렴성을 얻기 위한 업데이트이다.

Policy 는 보통 주어진 state 에서 가능한 action 들에 대한 확률함수로 주어지지만, 여기서는 편의상 behavoir policy 를 다음과 같은 형태의 함수로 정의하자.

$$\beta : \mathscr{S} \rightarrow \mathscr{A}.$$

즉, 주어진 state $s$ 에 대한 함숫값 $\beta(s) \in \mathscr{A}$ 는 $s$ 에서 policy $\beta$ 가 선택하는 action 이 된다.
다시한번 강조하자면 off-policy control 은 behavior policy $\beta$ 의 선택을 통하여 진행된 경험으로 얻은 reward 를 바탕으로 target policy $\pi$ 의 action value 함수에 해당하는 target variable $Q(s, a)$ 를 업데이트한다. 구체적으로 target policy 의 업데이트는 다음과 같은 형태로 이루어진다.

$$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha\left(R_{t+1} + \gamma \max_{a'} Q(S_{t+1},a') - Q(S_t, A_t)\right).$$

Behavior policy $\beta$ 에 의해 생성된 에피소드에서 timestep $t$ 에서의 state 가 $S_t = s$, 선택된 action 이 $A_t = a$ 라고 할 때 $Q(s, a)$ 의 값을 
좀 더 optimal 에 가까운 값으로 업데이트 하려는 의도인데, 그 방법이 현재의 값과 실제로 얻을 수 있는 값과의 차이를 계산해서 이를 반영하게 다는 것이다.
즉 현시점에서의 (tempoal) 차이 (difference) 를 근거로 삼아 업데이트 한다는 전략이다.

그러면 실제 optimal 한 acion value 는 어떤 값일까? 모델을 가정하여 상상해보면

$$q^\star(s, a) = \mathbb{E}_{\pi}(R_{t+1}+ \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots \vert S_t=s, A_t = a)$$ 

임을 알 수 있다. 하지만 우리는 모델을 이용하지 않고 이 값에 "가까운" 값을 계산하여 대신하려고 한다.
Sampling 을 통한 반복적인 업데이트로으 수렴성을 얻으려는 전략이기 때문에 expectation 부분은 점진적으로 해결이 된다고 보면, 결국

$$R_{t+1}+ \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots$$ 

에 최대한 가까운 값을 계산할 수 있으면 될 것 같다.

그렇다면 behavior policy $\beta$ 를 이용하여 action 을 취해나가면서 reward 를 긁어모아야 하나? 라고 생각할 수 있지만.. TD($0$) 의 아이디어는 
이 시점에서 bootstapping 을 도입하자는 것이다. 구체적으로, $\beta$ 에 의해 선택된 $a$ 로 인해 이미 $R_{t+1}$ 의 값은 이미 획득했고, 

$$\gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots = \gamma (R_{t+2} + \gamma R_{t+3} + \cdots) = \gamma G_{t+1}$$

임을 감안하면

$$R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) \approx R_{t+1}+ \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots$$

일거라는 예상을 할 수 있다. 즉, $\beta$ 를 통한 sampling 로 얻은 값 $R_{t+1}$ 에 직전까지 업데이트 되어있던 값 $Q(S_{t+1}, A_{t+1})$ 으로
bootstrapping 을 하면 될 것 같다. 하지만 여기서 한가지 결정적인 이슈가 발생한다

그 이슈란 바로 state $S_{t+1} = s'$ 에서 어떻게 다음 action 을 선택할 것인가의 문제이다. Off-policy control 을 지향하기 떄문에
target policy $\pi$ 는 이용할 수 없다. 그렇다고 behavior policy $\beta$ 를 이용하면 두 policy 간의 차이로 인한 variance 를 고려해야만 한다.
결론적으로, Q-leaning 의 선택은 $S_{t+1} = s'$ 에서 취할 수 있는 모든 action $a'$ 각각에 대한 값 $Q(s', a')$ 을 모두 들여다본 후,
그 중에 제일 큰 값 $\max_{a' \in \mathscr{A}} Q(s', a')$ 를 취하는 것이다.
즉,  

$$R_{t+1} + \gamma Q(S_{t+1}, {\rm argmax}_{a' \in \mathscr{A}} Q(s', a')) \approx R_{t+1}+ \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots$$

의 선택을 하겠다는 것이다. 
(참고로 최대값 대신 평균을 취하는 선택을 하는 전략은 Expected Sarsa 라고 불린다.)

종합하면 처음에 나왔던 업데이트 식

$$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha\left(R_{t+1} + \gamma \max_{a'} Q(S_{t+1},a') - Q(S_t, A_t)\right)$$

를 얻게된다.

---

### Deep Q-learning with experience reply

**Algorithm.**  
initialize $D$ to capacity $N$  
initialize $Q$ with random weights $\theta$  
initialize $\hat{Q}$ with weights $\theta^- = \theta$  
$\texttt{for}$ $episode$ in $\texttt{range}(1, M)$:  
$\,\,\,\,\,$ Initialize $s_1 = \\{x_1\\}$  
$\,\,\,\,\,$ Initialize $\phi_1 = \phi(s_1)$  
$\,\,\,\,\,$ $\texttt{for}$ $t$ in $\texttt{range}(1,T)$:  
$\,\,\,\,\,$ $\,\,\,\,\,$ $\texttt{if} \, coin_\varepsilon == 0$:  
$\,\,\,\,\,$ $\,\,\,\,\,$ $\,\,\,\,\,$ select a random $a_t \in \mathscr{A}$  
$\,\,\,\,\,$ $\,\,\,\,\,$ $\texttt{else}$:  
$\,\,\,\,\,$ $\,\,\,\,\,$ $\,\,\,\,\,$ $a_t = \arg\max Q_\theta(\phi(s_t),a)$  
$\,\,\,\,\,$ $\,\,\,\,\,$ execute action at in emulator and   
$\,\,\,\,\,$ $\,\,\,\,\,$ observe $r_t$ and $x_{t+1}$  
$\,\,\,\,\,$ $\,\,\,\,\,$ $s_{t+1} = s_t a_t x_{t+1}$    
$\,\,\,\,\,$ $\,\,\,\,\,$ $\phi_{t+1} = \phi(s_{t+1})$   
$\,\,\,\,\,$ $\,\,\,\,\,$ $D \leftarrow (\phi_t, a_t, r_t, \phi_{t+1})$  
$\,\,\,\,\,$ $\,\,\,\,\,$ sample random minibatch $(\phi_j, a_j, r_j, \phi_{j+1})$ from $D$    
$\,\,\,\,\,$ $\,\,\,\,\,$ $\texttt{if}$ $episode$ terminates at step $j+1$:  
$\,\,\,\,\,$ $\,\,\,\,\,$ $\,\,\,\,\,$ $y_j = r_j$  
$\,\,\,\,\,$ $\,\,\,\,\,$ $\texttt{else}$:  
$\,\,\,\,\,$ $\,\,\,\,\,$ $\,\,\,\,\,$ $y_j = r_j + \gamma \max_{a'} Q^-_{\theta^-}(\phi_{j+1}, a')$  
$\,\,\,\,\,$ $\,\,\,\,\,$ gradient descent on $(y_j - Q_\theta(\phi_j, a_j))^2$   
$\,\,\,\,\,$ $\,\,\,\,\,$ every $C$ steps reset $\theta^- = \theta$  

**Why experience replay?** 

**Why double networks?**
Deep reinforcement learning 이 tabular reinforcement learning 과 구별되는 중요한 차이 중 하나는 action value 함수의 argument 가 
state, action 에 더해 하나 더 추가된다는 것이다. Q-learning 의 경우 네트워크를 구성하는 CNN 파라미터 $\theta$ 가 그것이다.
구체적으로 CNN 은 $\theta$ 에 대한 다음과 같은 함수를 생성하는 역할을 한다.

$$\theta \mapsto Q(\,\cdot \, , \cdot \,; \theta) : \mathscr{S} \times \mathscr{A} \rightarrow \mathbb{R}.$$

즉, 각 $\theta$ 값은 하나의 state-action value 함수에 대응되는데 이 함수는 임의의 state-action pair $(s, a)$ 에 대한 action value 를 출력한다.
일반적으로, $\theta$ 의 값이 달라지면 같은 $(s, a)$ 에 대해서라도 다른 값을 출력하게 된다.

하나의 네트워크로 Q-learning 알고리즘을 구성하는 생각해보자. 

---

#### 참고자료

[1] R. Sutton, A. Barto, *Reinforcement leaning: an introduction*, second edition ([final draft](http://incompleteideas.net/book/the-book-2nd.html){:target="_blank"}).  
[2] V. Mnih et al., *Human-level control through deep reinforcement learning*, 
[Nature vol. **518**, 529–533](https://www.nature.com/articles/nature14236){:target="_blank"} (26 February 2015).

---

*읽으시다 오류나 부정확한 내용을 발견하시면 꼭 알려주시길 부탁드립니다. 감사합니다.*  
