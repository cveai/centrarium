---
layout: post
title:  "Notes on Convex Optimization"
date:   2018-03-09 00:00:00
author: 장승환
categories: Notes
tags: Convex Optimization
---

*In this page I summarize in a succinct and straighforward fashion what I learn from [Convex Optimization](https://lagunita.stanford.edu/courses/Engineering/CVX101/Winter2014/about){:target="_blank"} course by Stephen Boyd, along with my own thoughts and related resources.*
*I will update this page frequently, like every week, until it's complete.*

---

**Acronyms**
* LA: Linear Algebra

---

Preceding materials to be added..

---

#### (Lecture 9) Numerical linear algebra background

The main goal of the third and the last section (Lectures 9--12) of the course is to *demystify* how it is that we solve these convex optimization problems.

It is very important for everyone to know a little bit about how these problems are solved, and specifically the link between problem structure and how fast we can solve it.

**Matrix structure and algorithm complexity**

Cost of solving linear system $Ax = b$ ($A \in \mathbb{R}^{n\times n})$ :
* $n^3$, for general methods
* less, if $A$ is structured (banded, [sparse](https://www.quora.com/What-is-a-sparse-matrix-What-are-the-advantages-and-disadvantages-of-a-sparse-matrix){:target="_blank"}, Toeplitz, etc)

FLOP counts

**Linear equations that are easy to solve**
* Diagonal : $n$ flops 
* Lower triangular : $n^2$ flops via forward substitution
* Upper triangular : $n^2$ flops via backward substitution
* Orthogonal : $2n^n$ flops
* Permutation : $0$ flops

**The factor-solve method for solving $Ax = b$**
* Factor
* Solve

"Factorization dominates everything!"

**$LU$ facorization**
(completely equivalent to *Gaussian elimination*)

**Cholesky factorization**

**$LDL^{\rm T}$ factorization**

**Equations with structured sub-blocks**

**Structured matrix plus low rank term**

---

#### (Lecture 10) Unconstrained minimization

**Descent methods**

**Gradient descent**

**Steepest descent method**

**Newton's method**

[Newton's method](https://youtu.be/U0xlKuFqCuI) by G. Strang  
[(ML 15.1) Newton's method (for optimization) - intuition](https://youtu.be/28BMpgxn_Ec)  
[(ML 15.2) Newton's method (for optimization) in multiple dimensions](https://youtu.be/42zJ5xrdOqo)  

Newton step: $\Delta x_{\rm nt} = - \nabla f(x)^{-1}\nabla f(x)$
* $x + \Delta x_{\rm nt}$ minimizes 2nd order approximation: 

$$\hat{f}(x + v) = f(x) + \nabla f(x)^T v +\frac{1}{2}v^T \nabla^2f(x)v$$

<figure>
<img src="/assets/pics/conv-opt/nt1.png" alt="Newton step interpretation 1" style="width: 50%; height: 50%">
<figcaption>Newton step interpretation 1
</figcaption>
</figure>

* $x + \Delta x_{\rm nt}$  solves linearized optimality condition:

$$\nabla f(x + v) \approx \nabla\hat{f}(x = v) = \nabla f(x) + \nabla^2 f(x)v = 0$$

<figure>
<img src="/assets/pics/conv-opt/nt1.png" alt="Newton step interpretation 2" style="width: 50%; height: 50%">
<figcaption>Newton step interpretation 2
</figcaption>
</figure>

---

$$ $$

*To be added..*

---


