---
layout: post
title:  "MDP (Markov Decision Process)"
date:   2018-03-01 00:00:00
author: 장승환
categories: 기계학습
tags: 강화학습 RL
---

논의를 시작하기 전에 지난번에 살펴보았던 확률변수의 개념에 대해 몇 가지 보충해야 할 사항이 있다.
사실 지난 포스트 [/확률변수를 이해하다/](https://cveai.github.io/%EA%B8%B0%EA%B3%84%ED%95%99%EC%8A%B5/2018/02/14/rvariable.html)에서는 
이산 확률변수 중에서도 표본공간이 **유한**(finite)인 경우만을 다루었다. 
유한의 경우에 다루었던 모든 내용이 무한이면서 이산인 경우로 일반화되는데, 달라지는 거의 유일한 포인트는 극한 개념을 도입해야 한다는 것이다. 
극한의 개념이 요구되는 이유를 두가지로 생각해볼 수 있다. 
* 첫째, 확률변수의 기반이 되는 표본공간이 무한일 경우. 
* 둘째, 새로운 확률변수를 무한개의 확률변수의 합으로 정의할 때.

첫번째 경우는 모든 항 $a_n \ge 0$이고 급수의 합이 1로 수렴하는 수열 $a_n$을 제시하는 것과 본질적으로 같다.
$f(n) = a_n$ 으로 정의하면 $\Omega = \\{1, 2, ... \\}$를 표본공간으로 하는 확률분포함수를 주게된다. 

두번째의 경우는 MDP의 개념을 이해해가면서 구체적으로 다루기로 하자. 

확률변수에 대해 강조할 것 두가지:
* 함숫값이 실수가 아니어도 된다.
* 연산 및 합성해도 확률변수
* 확률변수는 확률적 성격 혹은 불확실성을 가진 데이터 (측정에 불확실성도 포함)



#### 마코프 (Markov)



#### 마코프 모델 (Markov Model)

시간에 따라 변화하는 데이터를 수학적으로 기술할 때 (거의 필수적으로) 사용되는 것이 마코프 모델이다.
물론 시간순의 데이터를 모델링하는 중요한 이유 중 하나는 (미래에 대한) 예측이다.


#### MDP (MArkov Decision Process)

상황설명


확률변수로 바로 들어감



MDP의 개념은 다음의 다섯가지의 기본 요소를 명확히 이해하면 어렵지 않게 머릿속에 그림을 그릴 수 있다.

* Agent
* Environment
* Action
* State
* Reward 


MDP 모델을 조금 쉽게 이해하기 위해 두 주체 간의 게임으로 이해해보자.
이 게임은 주인공인 agent와 주인공이 모험을 할 수 있도록 배경을 설정해주는 environment 간의 상호작용이다.
게임은 이산적인 time step (편의상 시점이라고 부르겠다) 을 따라가며 상호작용이 단계적으로 이루어진다.
좀 더 정확하게 표현해보면 시점을 나타내는 파라미터 $t$가 $t=0$, $t=1$, $t=2$, $\ldots$ 이렇게 변해가면서
agent와 environment가 각자의 활동을 하며 상호작용을 이어가게 된다.
각 시점에서 agent가 할 수 있는 선택을 action이라고 한다.
Agent가 게임에서 취할 수 있는 모든 가능한 action을 다 모아놓은 집합을 $\mathscr{A}$라고 하고, 
시점 $t$에 취하는 action을 $A_t$라고 표시하자. 자연스럽게 $A_t \in \mathscr{A}$임을 알 수 있다.
반면, 각 시점에서 environment는 두가지 반응으로 agent의 action에 대응하는데 그것이 각각 state와 reward이다.
Action의 경우와 마찬가지로 시점 $t$에 environment가 내놓게 되는 state를 $S_t$, reward를 $R_t$로 표시한다.
마찬가지로 $\mathscr{S}$와 $\mathscr{R}$는 각각 가능한 state들과 reward들의 집합을 나타낸다.






#### 환경 (Environment)

1992년에 출시된 [던전키퍼](https://web.archive.org/web/20020425231256/http://dk2.ea-europe.com/uk/dk1/index.html)라는 조금 철이 지난 비디오 게임이 있다.







#### 참고자료

[1]  R. Sutton, A. Barto, *Reinforcement leaning: an introduction*, second edition ([final draft](http://incompleteideas.net/book/the-book-2nd.html)).




[1] A. Shirayaev (translator: D. Chibisov), *Probability- 1*, third edition (2016), Springer.  
[2] 김민형, *확률론의 선과 악: 2. 확률론의 기원* ([네이버티비 동영상](http://tv.naver.com/v/1402550)).  
[3] Mathematical monk, *Probability primer* ([유튜브 동영상](https://www.youtube.com/watch?v=Tk4ubu7BlSk&list=PL17567A1A3F5DB5E4)).  
[4] PennStae, Eberly College of Science, *STAT 414: Probability theory* ([온라인 코스](https://onlinecourses.science.psu.edu/stat414/)).


---

*읽으시다 오류나 부정확한 내용을 발견하시면 꼭 알려주시길 부탁드립니다. 감사합니다.*  
(권 경모님, 권 휘님, 박 진우님, 손 주형님, 이 규복님, 김 기철님 감사드립니다.)


---

*읽으시다 오류나 부정확한 내용을 발견하시면 꼭 알려주시길 부탁드립니다. 감사합니다.*  
(권 경모님, 권 휘님, 박 진우님 감사드립니다.)
