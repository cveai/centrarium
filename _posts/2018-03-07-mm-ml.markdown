---
layout: post
title:  "Notes on Machine Learning"
date:   2018-03-08 00:00:00
author: 장승환
categories: Notes
tags: ML
---

*In this page I summarize in a succinct and straighforward fashion what I learn from [Machine Learning](https://www.youtube.com/watch?v=yDLKJtOVx5c&list=PLD0F06AA0D2E8FFBA){:target="_blank"} course by Mathematical Monk, along with my own thoughts and related resources.*
*I will update this page frequently, like every other day, until it's complete.*

---

**Acronyms**
* MM: Mathematical Monk
* ML: Machine Learning
* SL: Supervised Learning
* UL: Unsupervised Learning

* PSD: Positive Semi-Definite

* MCTC: Markov Chain Monte Carlo

---

* To understand **bias-variance "trade-off"**, take a quick route:  
(11.5) $\leadsto$ (11.1) (11.2) (11.3) (11.4) $\leadsto$ (11.1) (11.2) 

Some other helpful resources:
* [Lecture 08 - Bias-Variance Tradeoff](https://youtu.be/zrEyxfl2-a8) of Caltech's Machine Learning Course - CS 156 (Spring 2012) by Professor Yaser Abu-Mostafa


---

#### (ML 1.1) What is machine learning?

"Designing *algorithms* for inferring what is unknown from knowns."

MM considers ML as a subfield of statistics, with emphasis on algorithms.

Got to read an interesting article [Machine Learning vs. Statistics](https://svds.com/machine-learning-vs-statistics/){:target="_blank"}, thanks to [Whi Kwon](https://whikwon.github.io/){:target="_blank"}.

**Applications**
 * Spam (filtering out)
 * Handwriting (recognition)
 * Google streetview
 * Netflix (recommendation systems)
 * Navigation
 * Climate modelling

---

#### (ML 1.2) What is supervised learning?

 Classification of ML problems: Supervised vs. Unsupervised
 
**Supervised**: Given $(x^{(1)}, y^{(1)}), \ldots, (x^{(n)}, y^{(n)})$ choose a *function* $f(x) = y$.
 * Classification: $y^{(i)} \in \\{$finite set$\\}$
 * Regression: $y^{(i)} \in \mathbb{R}$ or $y^{(i)} \in \mathbb{R}^d$

<figure>
<img src="/assets/pics/mm-ml/classification.png" alt="Classification" style="width: 30%; height: 30%">
<figcaption>Classification
</figcaption>
</figure>

<figure>
<img src="/assets/pics/mm-ml/regression.png" alt="Regression" style="width: 70%; height: 70%">
<figcaption>Regression
</figcaption>
</figure>

* $x^{(i)}$ : data point
* $y^{(i)}$ : class/value/label

---

#### (ML 1.3) What is unsupervised learning?

Much less well-defined.

**Unsupervised**: Given $x^{(1)}, \ldots, x^{(n)}$, find *patterns* in the data.
* Clustering (typical UL)
* Density estimation (much more well-defined)
* Dimensionality reduction
* Feature leraning
* many more

<figure>
<img src="/assets/pics/mm-ml/clustering.png" alt="Clustering" style="width: 40%; height: 40%">
<figcaption>Clustering
</figcaption>
</figure>

<figure>
<img src="/assets/pics/mm-ml/de-dr.png" alt="Density estimation and Dimensinality reduction" style="width: 80%; height: 80%">
<figcaption>Density estimation and Dimensinality reduction
</figcaption>
</figure>

---

#### (ML 1.4) Variations on supervised and unsupervised
* Semi-supervised
* Active Learning
* Decision theory
* Reinforcement Learning
 
---

#### (ML 1.5) Generative vs discriminative models

Given data $(x^{(1)}, y^{(1)}), \ldots, (x^{(1)}, y^{(1)})$.  
Denote $(x, y)$ by a prototypical (data point, label) pair.

**Discriminative:** model $p(y\vert x)$ 

**Generative:** model the joint distribution 

$$
\begin{aligned}
p(x, y) &= f(x\vert y)p(y) \\
        &= p(y\vert x)f(x)
\end{aligned}$$

Some good reasons to use discriminative model: 
Statistically, it's very hard to estimate either $f(x\vert y)$ or $f(x)$ because it take a lotof data. (You're inclined to make mistakes.)

Generative process: 

---

#### (ML 1.6) $k$-Nearest Neighbor (kNN) classification algorithm 

Given (a training) data (set) $D = \\{(x_1, y_1), \ldots, (x_n, y_n)\\}$, $x_i \in \mathbb{R}, y_i \in \\{0, 1\\}$ and given a new data point.

Classify by deciding on what is $y$ corresponding to $x$ according to the majority vote from the $k$-nearest points in the training data.

(*Nearest* in terms of a pre-determined metric.)

<figure>
<img src="/assets/pics/mm-ml/knn.png" alt="3NN" style="width: 50%; height: 50%">
<figcaption>3NN in Euclidean metric
</figcaption>
</figure>

**Probabilistic formulation** (<span style="color:red">Discrimitive model!</span>) Fix $D$, $x$, $k$.

Find a RV $Y \sim p$ where $p(y) =$ #$\\{x_i \in N_k(x) : y_i = y \\}/k$ 

Sometimes people write $p(y\vert x, D)$ for $p(y)$.

The estimate (or prediction) is given by 

$$\hat{y} = \arg\max_y p(y\vert x, D).$$

How does one choose $k$? $\rightarrow$ Important problem of choosing parameters. (Bias-variance trade-off)

---

#### (ML 2.1) Classification trees (CART)

CART ([Classification And Regression Trees](https://books.google.co.kr/books/about/Classification_and_Regression_Trees.html?id=JwQx-WOmSyQC&redir_esc=y)) by Breiman et. al.   
(see: https://rafalab.github.io/pages/649/section-11.pdf)

Conceptually very simple approach to classification and regression.  
Can be extremely powerful, specially coupled with some randomizaiton technique, and essentially give the best performance.

**Main idea:** Form a *binary tree* (by binary splits), and *minimize error* in each leaf.

**Example.** (Classification tree)

Data set: $D = ((x_1, y_1), \ldots, (x_n, y_n))$ ($x_i \in \mathbb{R}^2, y \in \\{0, 1\\}$. 

New data point: <span style="color:green">$x$</span>

<figure>
<img src="/assets/pics/mm-ml/bin-split.png" alt="Binary splits for classitication tree" style="width: 50%; height: 50%">
<figcaption>Binary splits for classitication tree
</figcaption>
</figure>

<figure>
<img src="/assets/pics/mm-ml/bin-tree.png" alt="Binary classitication tree" style="width: 80%; height: 80%">
<figcaption>Binary classitication tree
</figcaption>
</figure>

The process defines a function <span style="color:green">$y = f(x)$</span> that is *constant* on each of the petitioned regions.

---

#### (ML 2.2) Regression trees (CART)

Regression tree ($x_i \in \mathbb{R}, y_i \in \mathbb{R}$)

<figure>
<img src="/assets/pics/mm-ml/bin-reg-split.png" alt="Binary splits for regression tree" style="width: 50%; height: 50%">
<figcaption>Binary splits for regression tree
</figcaption>
</figure>

$\hat{y} = \arg\max_y \sum_{i \in R_1}(y - y_i)^2$

$\Rightarrow \hat{y} =$ average of the $y_i$'s

<figure>
<img src="/assets/pics/mm-ml/bin-reg-tree.png" alt="Binary regression tree" style="width: 35%; height: 35%">
<figcaption>Binary regression tree
</figcaption>
</figure>

The process defines a function <span style="color:green">$y = f(x)$</span> that is *piecewise constant*.

---

#### (ML 2.3) Growing a regression tree (CART)

"Greedy" approach.

**First split.** Choose $j$ and $s$ to minimize the following:

$$\min_y \sum_{i:x_{ij}>s} (y-y_i)^2 + \min_y \sum_{i:x_{ij}\le s} (y-y_i)^2$$

**Splitting region $R$.**  Choose $j$ and $s$ to minimize the following:

$$\min_y \sum_{i:x_{ij}>s, x_i \in R} (y-y_i)^2 + \min_y \sum_{i:x_{ij}\le s, x_i \in R} (y-y_i)^2$$

<figure>
<img src="/assets/pics/mm-ml/grow-regression.png" alt="Growing regression tree" style="width: 35%; height: 35%">
<figcaption>Growing a regression tree
</figcaption>
</figure>

Stopping criteria::
1. Stop when only one point in $R$.
2. Only consider splits resulting in regions with $\ge m$ (say $m=5$) points per region.

Typically, people use "pruning" strategy.  
We're gonna take "random forest" approach instead.

---

#### (ML 2.4) Growing a classification tree (CART)

Datat set: $(x_1, y_1), \ldots, (x_n, y_n)$

$E_R =$ fraction of points $x_i \in R$ misclassified by a majority vote in $R$  
$\, \, \, \, \, \, \, \, $ $= \min_y \frac{1}{N}\sum_{i: x_i \in R} I(y_i\neq y)$, $N_R =$ #$\\{i : x_i \in R\\}$

**First split.** Choose $j$ and $s$ to minimize the following:

$$N_{R_1(j,s)}E_{R_1(j,s)} + N_{R_1'(j,s)}E_{R_1'(j,s)}$$

where $R_1(j, s) = \\{x_i : x_{ij} >s\\}, R_1'(j, s) = \\{x_i : x_{ij} \le s\\}$

Let $R_2 = R_1(j, s), R_3 = R_1'(j,s)$

**Splitting $R_k$.** Choose $j$ and $s$ to minimize the following:

$$N_{R_k(j,s)}E_{R_k(j,s)} + N_{R_k'(j,s)}E_{R_k'(j,s)}$$

where $R_k(j, s) = \\{x_i \in R_k : x_{ij} >s\\}, R_k'(j, s) = \\{x_i \in R_k : x_{ij} \le s\\}$

<figure>
<img src="/assets/pics/mm-ml/grow-classification.png" alt="Growing classification tree" style="width: 35%; height: 35%">
<figcaption>Growing a classification tree
</figcaption>
</figure>

Stopping criteria:
1. Stop when only one point in $R$.
2. Only consider splits resulting in regions with $\ge m$ (say $m=5$) points per region.
3. Stop when $R_k$ contains only points of one class.

Use when minimizing?
1. Entropy
2. Gini index

---

#### (ML 2.5) Generalizations for trees (CART)

**Imputiry measures for classification:**
1. Misclassification rate $E_R$
2. Entropy

$$H_R = - \sum_{y \in \mathscr{Y}}p_R(y) \log p_R(y)$$

where $\mathscr{Y}$ is a finite set of (possible) classes and $p_R$ empirical distribution.

The idea of entropy : want to choose splits in which each of the regions are as homogenous as possible.

3. "Gini index" 

$$H_R = \sum_{y \in \mathscr{Y}}p_R(y) (1 - p_R(y))$$

**Remark**
1. $H_R$ and $G_R$ tend to work better than $E_R$.
2. $G_R$ has some nice analytical properties that makes it easier to work with.

HTR ([The Elements of 
Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/) by [Trevor Hastie](http://www-stat.stanford.edu/~hastie/), [Robert Tibshirani](http://statweb.stanford.edu/~tibs/), [Jerome Friedman](http://statweb.stanford.edu/~jhf/))

* Categorical predictors
* Loss matrix
* Missing values
* Linear combinations
* Instability

---

#### (ML 2.6) Bootstrap aggregation (Bagging)

A fantastic technique for taking a classifier and making it better (By Breiman)

**Bagging for Regression.**

$D = \\{ (X_1^{(1)}, Y_1^{(1)}), \ldots, (X_n^{(1)}, Y_n^{(1)}) \\}$ $\sim P$ iid  

Given a new point $x$, predict <span style="color:orange">$Y^{(1)}$</span> (<span style="color:red">$f(x) = y$</span>)

From the given sample, uniformly sample with replacement to obtain:

$(X_1^{(2)}, Y_1^{(2)}), \ldots, (X_n^{(2)}, Y_n^{(2)})$ <span style="color:orange">$Y^{(2)}$</span>

$\cdots$

$(X_1^{(m)}, Y_1^{(m)}), \ldots, (X_n^{(m)}, Y_n^{(m)})$ <span style="color:orange">$Y^{(m)}$</span>

<figure>
<img src="/assets/pics/mm-ml/bagging-regression.png" alt="Bagging for regression" style="width: 35%; height: 35%">
<figcaption>Bagging for regression
</figcaption>
</figure>

Suppose each esimiator $Y^{(i)}$ has teh correct mean, i.e., 

$EY^{(i)} = y = f(x)$ for each $i \\{1, \ldots, m\\}$

In other words, they are "unbiased estimators."

Now we measure our error according to the squared distance from the true value $(Y-y)^2$, which is called a *loss function*.

Then the *risk* (i.e. expected loss) is given by

$$E((Y-y)^2) = E((Y-EY)^2) = \sigma^2(y)$$

If we define a new RV (this is where the *aggregation* comes in!)

$$Z = \frac{1}{m}\sum_{i=1}^m Y^{(i)}$$

we get $EZ = \frac{1}{m}\sum y = y$. ($Z$ is also an unbiased estimator!)

Then

$$\begin{aligned}
E((Z-y)^2) &= E((Z-EZ)^2) = \sigma^2(Z) = \sigma^2(\frac{1}{m}\sum Y^{(i)}) \\
           &= (\frac{1}{m})^2\sigma^2(\sum Y^{(i)}) = \frac{1}{m^2}\sum\sigma^2(Y^{(i)}) = \frac{1}{m}\sigma^2(Y)
\end{aligned}$$

Because we just have one data set, we approximae $P$ by the empirical distribution $\hat{P}$ to draw bootstrap samples

$$(X_i^{(k)}, Y_i^{(k)}) \sim$$ Uniform$(D)$ iid. 

---

#### (ML 2.7) Bagging for classification

Two essential approaches:
1. **Majority vote.** For each data set construct a classifier to get a sequence of classifiers $C_1, \ldots, C_n$. Then given a new point $x$, look at the class that each $C_i$ predicted for $x$ and take majority vote.
2. **Average estimated probabilities.** Th classifiers define $p_x^{(1)}, \ldots, p_x^{(n)}$, (estimated) PMFs on $\mathscr{Y}$. We define


$$\hat{p}_x(y) = \frac{1}{m}\sum_{i=1}p_x^{(i)}(y)$$

(similar to the regerssion case)

We classify $x$ as the most likely class according to this estimated probability.


**Genralization**
Now let's see what we can do by dropping the two assumptions <span style="color:red">$f(x)=y$</span> and <span style="color:red">"unbiasedness of classifiers"</span> and only keeping the <span style="color:blue">iid</span> assumption.

Assume we have a new point $x$ and a RV $W$ (true value).

<figure>
<img src="/assets/pics/mm-ml/bagging-classification.png" alt="Dropping assumptions" style="width: 70%; height: 70%">
<figcaption>Dropping assumptions
</figcaption>
</figure>

---

#### (ML 2.8) Random forests

An extremely simple technique with state of the art perfomance

A study by Caruana & Niculescu-Mizil (2006), [An Empirical Comparison of Supervised Learning Algorithms](https://www.cs.cornell.edu/~caruana/ctp/ct.papers/caruana.icml06.pdf), shows:
1. Boosted decition free (another aggregation tree)
2. Random forests
3. Bagged decision tree
4. SVM
5. Neural Nets
$\cdots$

$D = ((x_1, y_1), \ldots, (x_n, y_n)$

For $i$ in [1, 2, .. , B]:  
$\,\,\,\,\,\,\,\,\,$ Choose bootstrap sample $D_i$ from $D$  
$\,\,\,\,\,\,\,\,\,$ Construct tree $T_i$ using $D$ s.t.  
$\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,$ At each note, choose random subset of $m$ features, and only consider splitting on thoese features  

---

$\cdots$

---

#### (ML 3.5) The Big Picture (part 1)

**Problem:** Minimize 

$${\rm EL}(Y, f(X))$$

using the obsrved $p(y\vert x)$ as the key quantity.

* EL: Expected Loss
* $y$: true value
* $f(x)$: our prediction

Core concepts and methods in ML fall out naturally from trying to solve this problem.

Data: $D = ((x_1, y_), \ldots, (x_n, y_n))$

**Discriminative** Estimate $p(y\vert x)$ directly using $D$.
* $k$NN
* Trees
* SVM

**Generative** Estimate $p(y, x)$ using $D$, and then recover $p(y\vert x) = \frac{p(x, y)}{p(x)}$.

**Parameters / Latent variables** $\theta$, consider $p_\theta(x, y) = p(x, y \vert \theta)$

$$p(y\vert x, D) = \int p(y\vert x, \theta, x, D) p(\theta\vert x, D) d\theta$$

<figure>
<img src="/assets/pics/mm-ml/big-picture.png" alt="Big picture" style="width: 80%; height: 80%">
<figcaption>Big picture
</figcaption>
</figure>


---

#### (ML 3.6) The Big Picture (part 2)
1. Exact inference (usually not possible)
* Multivariate Gaussian (very nice), Conjugate priors, Graphical models (us DP)
2. Point estimate of $\theta$ (simplest)
* MLE, MAP (Maximum A Posteriori)  
* Optimization, EM (Expectation Maximization) / Empirical Bayes
3. Deterministic Approximation
* Laplace Approximation, Variational methods, Expextation Propagation
4. Stochastic Approximation
* MCTC (Gibbs sampling, MH), Importance Sampling (Particle filter)

---

#### (ML 3.7) The Big Picture (part 3)

**Density estimation** (Unsupervised)

$D = (X_1, \ldots, X_n), X_i \in \mathbb{R}^d$, iid.

Goal: Estimate the distribution.

---

#### (ML 4.1) Maximum Likelihood Estimation (MLE) (part 1)

Setup: Given data $D = (x_1, \ldots, x_n), x_i \in \mathbb{R}^d$.
 
Assume a set distributions $\\{p_\theta : \theta \in Theta \\}$ on $\mathbb{R}^d$. ()

Assume $D$ is a sample from $X_1, \ldots, X_n ~ p_\theta$, iid for some $\theta \in \Theta$.

Goal: Estimate the true $\theta$ that $D$ comes from.

**Definition.** $\theta_{\rm MLE}$ for $\theta$ if $\theta_{\rm MLE} = \arg \max_{\theta \in \Theta} p(D\vert \theta).$

(more precisely, $p(D \vert \theta_{\rm MLE}) = \max_{\theta \in \Theta}p(D \vert \theta)$)

$p(D \vert \theta) := p(x_1, \ldots, x_n \vert \theta) = \prod_{i = 1}^n p(x_i \vert \theta) = \prod_{i = 1}^n P[X_i = x_i \vert \theta]$

**Remark**  
(1) MLE might not be unique.  
(2) MLE may fail to exist.  

---

#### (ML 11.1) Estimators

Assume $D = (X_1, \ldots, X_n)$ where the $X_i$ are RVs

**Definition** A *statistic* is a RV $S$ that is a function of the data $D$. (i.e. S=f(D)

**Terminology** An *estimator* is a statistic intended to approximate a parameter governing the distribution of $D$.

**Notation** 
1. $\hat{\theta}$ denotes an estimator of $\theta$.
2. $\hat{\theta}_n$ emphasize (the dependence on) $n$

**Example** $X_1, \ldots, X_n \sim N(\mu, \sigma^2)$ iid  
(Sample mean) $\hat{\mu} = \hat{\mu}(X_1, \ldots, X_n) \frac{1}{n}\sum_{i=1}^nX_i$  (cf. $\sigma^2 = E((X - \mu)^2)$)
("Biased" sample variance) $\sigma^2 = \frac{1}{n}\sum_{i=1}^n(X_i -\bar{X})^2$  
("unbiased" sample variance) $\sigma^2 = \frac{1}{n-1}\sum_{i=1}^n(X_i -\bar{X})^2$

**Definition**
1. The *bias* of an estimator $\hat{\theta}$ is ${\rm bias}(\hat{\theta}) = E\hat{\theta} - \theta$.  
2. An estimator $\hat{\theta}$ is *unbiased* if ${\rm bias}(\theta) = 0$.  

**Example**
1. $\hat{\mu}$ is unbiased:    
$E\hat{\mu} = E\frac{1}{n}\sum_{i=1}^nX_i =\frac{1}{n}\sum_E X_i = \frac{1}{n} = \mu$  
2. $\hat{sigma}^2$ is biased. (Exercise)
3. $s^2$ is unbiased. (Exercise)

---

#### (ML 11.2) Decision theory terminology in different contexts

**General**             | **Estimators**                         | **$^*$Regression/Classification**
Decision rule $\delta$  | $^*$Estimator function $g$             | Prediction function $f$
State $s$ (unknown)     | Parameter $\theta$ (unknown)           | Target value $Y$ (unknown)
$^*$Data $D$ (observed) | Data $D$ (observed)                    | Point $X$ (observed)
Action $a = \delta(D)$  | Estimator/Estimate $\hat{\theta}=g(D)$ | Prediction $\hat{Y} = f(X)$
Loss $L(s, a)$          | Loss L(\theta, \hat{\theta})           | Loss $L(Y, \hat{Y})$

**Example**  
Estimator is a RV: $\hat{\mu} = \frac{1}{n} \sum_{i=1}^n X_i$  
Estimate is a number: $\hat{\mu} = \frac{1}{n} \sum_{i=1}^n x_i = 2.3$  
SIn some situation the procedure $g$ is refered to as an estimator!

---

#### (ML 11.3) Frequentist risk, Bayesian expected loss, and Bayes risk

Exciting session to clear up all the mud!

$D = (X_1, \ldots, X_n)$, $D \sim p_\theta$, $\hat{\theta} = f(D) = \delta(D)$ 
$\theta \sim \pi$ (the parameter $\theta$ is a RV)  
Everything begins with : Loss $=L(\theta, f(D))$
We wanna minimize the loss but it's an RV! Two option to deal with it:  
1. Averaging over $\theta$ given the data: $E(L(\theta, f(D)) \vert D =:\rho(\pi, f(D))$ (<span style="color:blue">Bayesian expected loss</span>) 
2. Averaging over the data given $\theta$: $E(L(\theta, f(D)) \vert \theta =: R(\theta, f)$ (<span style="color:red">(Frequentist) risk</span>) 


<figure>
<img src="/assets/pics/mm-ml/bayes-risk.png" alt="Bayes Risk" style="width: 100%; height: 100%">
<figcaption>
</figcaption>
</figure>

---

#### (ML 11.4) Choosing a decision rule - Bayesian and frequentist

<span style="color:orange">**How to choose $f$.**</span>  

<span style="color:blue">**Bayesian:** Assume $\pi$</span>  
<span style="color:blue">Case 1. Know $D$. Choose $f(D)$ to minimize $\rho(\pi, f(D))$</span>    
<span style="color:blue">Case 1. Know $D$. Choose $f(D)$ to minimize $\rho(\pi, f$</span>  

<span style="color:red">**Frequentist:** Introduce a furthere principle to guide your choice.</span>  
<span style="color:red">(a) Unbiasedness</span>  
<span style="color:red">(b) Admissibility</span> 
<span style="color:red">(a) Minimax</span>  
<span style="color:red">(a) Invariance</span>  

<figure>
<img src="/assets/pics/mm-ml/frequentist.png" alt="A frequentist approaches" style="width: 70%; height: 70%">
<figcaption>A frequentist approach
</figcaption>
</figure>

---

#### (ML 11.5) Bias-Variance decomposition (MSE $=$ bias$^2$ + var)

"A super impportant port of ML is what's called <span style="color:red">model selection</span> and a tool for model selection is the bias-variance decomposition."

Almost trivial identity but extremely handy.

**Definition.** Let $D$ be random data. The MSE of an estimator $\hat{\theta} = f(D)$ for $\theta$ is

$${\rm MSE}(\hat{\theta}) = E((\hat{\theta} - \theta)^2\vert \theta)$$

Put $\vert \theta$ emphasizing we're not averagning over $\theta$ here  (we don't have a distribution over $\theta$).
We're just averaging over the data. 

MSE$\theta$ is nothing but the risk $R(\theta, f)$ under square loss, i.e., when the loss function is the square of the deifference.

**Recall.** bias$(\hat{\theta}) = E(\theta) -\theta$.

**Proposition.** MSE$(\theta) = bias(\hat{\theta})^2 + {\rm var}(\hat{\theta})$

Proof:
<figure>
<img src="/assets/pics/mm-ml/proof-bias-variance.png" alt="Bias-variance" style="width: 70%; height: 70%">
<figcaption>
</figcaption>
</figure>

**Silly example.**
$X \sim N(\theta, 1)$
$\theta$ nonrandom & unknown
$D = X

"Natural" estimate of $\theta$: $\delta_1(D) = X \leadsto$ bias$^2 = 0$, var$ = 1$, MSE$ =1$  
"Silly" estimate of $\theta$: $\delta_0(D) = X \leadsto$ bias$^2 = \theta^2$, var$ = 0$, MSE$ = \theta^2$

cf. Shrinkage, Stein's paradox

---

#### (ML 12.1) Model selection - introduction and examples

<span style="color:orange">**"Model" selection**</span> really means "complexity" selection!

Here, *complexity* $\approx$ flexibility to fit/explain data  

**Example** (Linaer regression with MLE for $w$)  $f(x) = w^T\varphi(x)$  
Given data $x \in \mathbb{R}$, consider polynomial basis $\varphi(x) = x^k$, $\varphi = (\varphi_0, \varphi_1, \ldots, \varphi_B)$

Turns out <span style="color:purple">$B =$ "complexity parameter"</span>

<figure>
<img src="/assets/pics/mm-ml/model-selection.png" alt="Model selection illustration" style="width: 80%; height: 80%">
<figcaption>Model (in fact complexity) selection illustration
</figcaption>
</figure>

**Example** (Bayesia linear regression or MAP)
<figure>
<img src="/assets/pics/mm-ml/bayesian-regression.png" alt="Bayesian linear regression" style="width: 100%; height: 100%">
<figcaption>
</figcaption>
</figure>

**Example** ($k$NN)   
<figure>
<img src="/assets/pics/mm-ml/knn-model-selection.png" alt="Classification example" style="width: 80%; height: 80%">
<figcaption>
</figcaption>
</figure>
<span style="color:purple">$k$</span> "controls" decesion boundaties.


---

#### (ML 12.2) Bias-variance in model selection

Bias-variance trade-off, as they say.  
MSE $=$ bias$^2 +$ var  / $\in$MSE $=$ $\int$bias$^2 +$ $\int$var (only applies for <span style="color:green">square loss</span>)
<figure>
<img src="/assets/pics/mm-ml/bias-var-convex.png" alt="Bias-variance trade-off" style="width: 60%; height: 60%">
<figcaption>Bias-variance "trade-off"
</figcaption>
</figure>

<figure>
<img src="/assets/pics/mm-ml/bias-var-trade.png" alt="Bias-variance trade-off" style="width: 80%; height: 80%">
<figcaption>
</figcaption>
</figure>

---

#### (ML 15.1) Newton's method (for optimization) - intuition

2nd order method!

(Gradient descent $x_{t+1} = x_t - \alpha_t \nabla f(x_t)$ : 1st order method)

**Analogy (1D).**

* zero-finding: $x_{t+1} = x_t - \frac{f(x_t)}{f'(x_t)}$

<figure>
<img src="/assets/pics/mm-ml/zero-finding.png" alt="zero-finding" style="width: 35%; height: 35%">
<figcaption>
</figcaption>
</figure>

* min./maximizing: $x_{t+1} = x_t - \frac{f'(x_t)}{f''(x_t)}$

<figure>
<img src="/assets/pics/mm-ml/minimizing.png" alt="minimizing" style="width: 60%; height: 60%">
<figcaption> Minimizing in 1D
</figcaption>
</figure>

<figure>
<img src="/assets/pics/mm-ml/2d.png" alt="minimizing in 2D" style="width: 60%; height: 60%">
<figcaption> Minimizing in 2D
</figcaption>
</figure>

---

#### (ML 15.2) Newton's method (for optimization) in multiple dimensions

Idea: "Make a 2nd order approximation and minimize tha."

Let $f: \mathbb{R}^n \rightarrow \mathbb{R}$ be (sufficiently) smooth.

**Taylor's theorem:** for $x$ near a, letting $g = \nabla f(a)$ and $H = \nabla^2f(a) = \left(\frac{\partial^2}{\partial x_i \partial x_j}f(a)\right)_{ij}$,

$$\begin{aligned}
f(x) &\approx f(a) + g^T(x-a) +\frac{1}{2}(x-a)^TH(x-a) \\
     &= \frac{1}{2}x^THx + b^Tx +c =:q(x)
\end{aligned}$$

<figure>
<img src="/assets/pics/mm-ml/newton.png" alt="Newton's method" style="width: 35%; height: 35%">
<figcaption> 
</figcaption>
</figure>

Minimize: 
$0 = \nabla q = Hx + b \Rightarrow x = -H^{-1}b = a -H^{-1}g$

Critical to check: $\nabla^2 q = H$ $\Rightarrow$ minimum if $H$ is PSD.

**Algorithm.**
* Initialize $x \in \mathbb{R}^n$
* Iterate: $x_{t+1} = x_t - H^{-1}g$ where $g = \nabla f(x_t), H = \nabla^2 f(x_t)$

**Issues.**
1. $H$ may fail to be PSD. (Option: switch gradient descent. A smart way to do it: Levenberg–Marquardt algorithm) 
2. Rather than invert $H$, sove $Hy = g$ for $y$, then use $x_{t+1} = x_t - y$. (More robust approach)
3. $x_{t+1} = x_t - \alpha_t y$. (small "step size" $\alpha_t>0$)

---

#### (ML 17.1) Sampling methods - why sampling, pros and cons

Why sampling?
* For approximate expectations (estimate statistics / posterior infernce i.e. computing probability)
* For visualization

Why expectations?
* Any probability is an expectation: $P[X \in A] = E[I(X \in A)]$.
* Approximation is needed for intractable sums/integrals (can be expressed as expectations)

Pros.
* Easy (both to implement and understand)
* General purpose

Cons.
* Too easy - used inappropriately
* Slow
* Getting "good" samples may be dificult
* Difficult to assess

---

#### (ML 17.2) Monte Carlo methods - A little history

<figure>
<img src="/assets/pics/mm-ml/mc-history.png" alt="A little history of MC" style="width: 80%; height: 80%">
<figcaption>A little history of Monte Carlo methods
</figcaption>
</figure>

---

#### (ML 17.3) Monte Carlo approximation

Goal: Aprroximate $E[f(X)]$, when intractable.

Definition (Monte Carlo estimator): If $X_1, \ldots, X_n \sim p$ iid then 

$$\hat{\mu}_n = \frac{1}{n}\sum_{i=1}^nf(X_i)$$

is a (basic) *Monte Carlo estimator* of $E[f(X)]$ where $X \sim p$. (sample mean)

Remark  
(1) $E[\hat{\mu}_n] = E[f(X)]$ (i.e. $\hat{\mu}_n$ is an unbiased estimator)  
(2) 

---

#### (ML 17.5) Importance sampling - introduction
<span style="color:red">It's not a sampling method but an estimation technique!</span>

It can be though of as a variant of MC estimation.

Recall: MC estimation (by sample mean): 

$$E[f(X)] \approx \frac{1}{n}\sum_{i=1}^nf(X_i)$$

under the BIG assumtion that $X \sim p$ and $X_i \sim p$.

Can we do something similar by drawing samples from an alternative distribution $q$?

Yes, and in some cases you can do much much better!

($p$ density case)

$$E[f(X)] = \int f(x)p(x)dx = \int f(x)\frac{p(x)}{q(x)}p(x)dx \approx \frac{1}{n}\sum_{i=1}^nf(X_i)\frac{p(X_i)}{q(X_i)}$$

holds for all (pdf) $q$ s.t. $q(x)= 0 \Rightarrow p(x) = 0$, i.e., $p$ is absolutely continuous w.r.t. $q$.

<figure>
<img src="/assets/pics/mm-ml/importance-sampling.png" alt="Importance sampling" style="width: 80%; height: 80%">
<figcaption>Importance sampling
</figcaption>
</figure>

---

$$ $$

*To be added..*

---


