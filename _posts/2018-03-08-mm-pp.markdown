---
layout: post
title:  "Notes on Probability Primer"
date:   2018-03-08 00:00:00
author: 장승환
categories: Notes
tags: Probability
---

*In this page I summarize in a succinct and straighforward fashion what I learn from [Probability Primer](https://www.youtube.com/watch?v=Tk4ubu7BlSk&list=PL17567A1A3F5DB5E4){:target="_blank"} course by Mathematical Monk, along with my own thoughts and related resources.*
*I will update this page frequently, like every other day, until it's complete.*

---

**Acronyms**
* RV: random variable

---

#### (PP 1.1) Measure theory: Why measure theory - The Banach-Tarski Paradox

<figure>
<img src="/assets/pics/mm-pp/banach-tarski.png" alt="Why measure" style="width: 80%; height: 80%">
<figcaption>Why measure theory?
</figcaption>
</figure>

A bit more detailed explanations on the Banach-Tarski paradox here: [The Banach–Tarski Paradox](https://youtu.be/s86-Z-CbaHA){:target="_blank"}.

---

#### (PP 1.2) Measure theory: $\sigma$-algebras

**Definition.** 
Given a set $\Omega$, a *$\sigma$-algebra* on $\Omega$ is a nonempty collection $\mathcal{A} \subset 2^{\Omega}$ s.t.
1. closed under complements ($E \in \mathcal{A} \Rightarrow E^c \in \mathcal{A}$)
2. closed under countable unions ($E_1, E_2, \ldots \in \mathcal{A} \Rightarrow \cup_{i = 1}^\infty E_i \in \mathcal{A}$)

**Remark**
1. $\Omega \in \mathcal{A}$ (some $E \in \mathcal{A}$. $E^c \in \mathcal{A}$. $\Omega = E \cup E^c \in \mathcal{A}$.)
2. $\emptyset \in \mathcal{A}$ ($\emptyset = \Omega^c \in \mathcal{A}$)
3. $\mathcal{A}$ is closed under countable intersections:  
If $E_1, E_2, \ldots \in \mathcal{A}$, then (by De Morgan's laws)

$$\cap_{i=1}^\infty = \cap (E_i^c)^c = (\cup_{i=1}^\infty)^c \in \mathcal{A}$$

---

#### (PP 1.3) Measure theory: Measures

**Definition.** Given $\mathscr{C} \in 2^\Omega$, the *$\sigma$-algebra generated by $\mathscr{C}$, witten $\sigma(\mathscr{C})$*
is the "smallest" $\sigma$-algebra containing $\mathscr{C}$. (That is, $\sigma(\mathscr{C}) = \cap_{\mathcal{A} \supset \mathscr{C}} \mathcal{A}$)

**Remark.** $\sigma(\mathscr{C})$ always exists because:  
1. $2^\Omega$ is a $\sigma$-algebra
2. any intersection of $\sigma$-algebras is a $\sigma$-algebra  

**Examples.**  
1. $\mathcal{A} = \\{\emptyset, \Omega\\}$
2. $\mathcal{A} = \\{\emptyset, E, E^c, \Omega\\}$
3. If $\Omega = \mathbb{R}$ then the *Borel $\sigma$-algebra* is $\mathcal{B} = \sigma(\mathscr{T})$ 
where $\mathscr{T} =$ $\\{$open sets of $\mathbb{R} \\}$

**Definition.** A measure $\mu$ on $\Omega$ with $\sigma$-algebra $\mathcal{A}$ is a function $\mu : \mathcal{A} \rightarrow [0, \infty]$ s.t.
1. $\mu(\emptyset) = 0$
2. $\mu(\cup_{i=1}^\infty E_i) = \sum_{i=1}^\infty \mu(E_i)$ for any sequence $E_1, E_2, \ldots \in \mathcal{A}$ of pairwise disjoint sets (*coultable additivity*)

**Definition.** A *probability measure* is a measure $P$ s.t. $P(\Omega)= 1$

---

#### (PP 1.4) Measure theory: Examples of Measures

The defining conditions of a probability measure is called the **Kolmogorov's axioms**. (father of modern probability theory)

**Examples.**
1. (Finite set) $\Omega = \\{1, \ldots, n\\}, \mathcal{A} = 2^\Omega$, $P(\{k\}) = P(k) = \frac{1}{n}$ (gives *uniform distribution*)  
$P(\\{1, 2, 4\\}) = P(\\{1\\} \cup \\{2\\} \cup \\{4\\})) = P(1) + P(2) + P(4)$
2. (Countably infinite) $\Omega = \\{1, 2, 3, \ldots \\}, \mathcal{A} = 2^\Omega$  
$P(k) =$ probability it takes $k$ coinflips to get heads $=\alpha(1-\alpha)^{k-1} = \frac{1}{2}(1-\frac{1}{2})^{k-1}$  
(gives *gemetric distribution*)
3. (Uncountable) $\Omega = [0, \infty)$, $\mathcal{A} = \mathcal{B}([0, \infty \)])$  
$P([0, x)) = 1-e^{-x}$ $\forall x>0$ (gives *exponential distribution*)  
Note: $P(\\{x\\})$ $\forall x>0$ (a symptom of continuous distributions)

---

#### (PP 1.5) Measure theory: Basic Properties of Measures

(4) Lebesgue measure (on $\mathbb{R}$). $\Omega, \mathcal{A} = \mathcal{B}(\mathbb{R})$

$$\mu((a, b)) = b-1$$

for any $a, b \in \mathbb{R}, a< b$.

**Theorem (Basic properties of measures).** Let $(\Omega, \mathcal{A}, \mu)$ be a measure space.
1. Monoronicity: If $E, F \in \mathcal{A}$ and $E \subset F$, then $\mu(E) \le \mu(F)$.
2. Subadditivity (handy!): If $E_1, E_2, \cdots \in \mathcal{A}$, then $\mu(\cup_{i=1}^\infty E_i) \le \sum_{i=1}^\infty E_i$

---

#### (PP 1.6) Measure theory: Basic Properties of Measures (continued)

1. 
2. 
3. Continuity from below: If $E_1, E_2, \cdots \in \mathcal{A}$ and $E_1 \subset E_2 \subset \cdots$,
then $\mu(\cup_{i=1}^\infty E_i) = \lim_{i \rightarrow \infty} \mu(E_i)$
4. Continuity from above: If $E_1, E_2, \cdots \in \mathcal{A}$ and $E_1 \supset E_2 \supset \cdots$ and $\mu(E_1) < \infty$,
then $\mu(\cap_{i=1}^\infty E_i) = \lim_{i \rightarrow \infty} \mu(E_i)$

<figure>
<img src="/assets/pics/mm-pp/continuity-from-above.png" alt="Continuity from below" style="width: 35%; height: 35%">
<figcaption>Continuity from below
</figcaption>
</figure>

<figure>
<img src="/assets/pics/mm-pp/continuity-from-below.png" alt="Continuity from above" style="width: 35%; height: 35%">
<figcaption>Continuity from above
</figcaption>
</figure>

Properties 3 and 4 are very innocent looking but are essential in proving pretty nontrivial theorems!

---

#### (PP 1.7) Measure theory: More Properties of Probability Measures

**Facts.** Let $(\Omega, \mathcal{A}, P)$ be a probabilistic measure space with $E, F, E_i \in \mathcal{A}$.
1. $P(E \cup F) = P(E) + P(F)$ if $E \cap F = \emptyset$
2. $P(E cup F) = P(E) + P(F) - P(E \cap F)$
3. $P(E) = 1 - P(E)$
4. $P(E \cap F^c) = P(E) - P(E \cap F)$
5. (Inclusion-Exclusion formula) 
$P(\cup_{i=1}^n E_i) = \sum_{i} P(E_i) - \sum_{i<j}P(E_i \cap E_j) + \sum_{i<j<k}P(E_i \cap E_j \cap E_k) - \cdots 
+(-1)P(E_1 \cap E_2 \cap \cdots E_n)$
6. 


---

#### (PP 2.1) Conditional Probability

Conditinoal probability and independence are critical topics in applications of probability.

**Notation** "Suppress" $(\Omega, \mathcal{A})$.
Whenever write $P(E)$ is implicitly assmung underlying some ptobability measure sapce $\Omega$ and some $\sigma$-algebra $\mathscr{A}$ with some measure $p$ defined on these.

**Terminology**
* event = measureable set = set in $\mathcal{A}$
* sample space = $\Omega$

**Definition** Assuming $P(B) > 0 $, define the *conditional probability of $A$ given $B$* as

$$P(A \vert B) = \frac{P(A \cap B)}{P(B)}.$$

<figure>
<img src="/assets/pics/mm-pp/conditional.png" alt="Conditional probability" style="width: 80%; height: 80%">
<figcaption>Conditional probability
</figcaption>
</figure>

---

#### (PP 2.2) Independence

**Definition.**
Eventa $A< B$ are *independent* if $P(A \cap B) = P(A)P(B)$.

<figure>
<img src="/assets/pics/mm-pp/independence.png" alt="Independence" style="width: 30%; height: 30%">
<figcaption>Independence
</figcaption>
</figure>

**Definition.**
Eventa $A_1, \ldots, A_n$ are *(mutually) independent* if for any $S \subset \\{1, \ldots, n\\}$,

$$P(\cap_{i \in S} A_i) = \prod_{i \in S} P(A_i).$$

**Remark.** Mutual independence $\Rightarrow$ pairwise independence

**Warning!** Pairwise independence $\nRightarrow$ mutual independence

<figure>
<img src="/assets/pics/mm-pp/mutual.png" alt="Mutual independence" style="width: 40%; height: 40%">
<figcaption>Pairwise independence $\nRightarrow$ mutual independence
</figcaption>
</figure>

**Definition.** $A, B$ are *conditionally independent given $C$* (where $P(C) >0$) if 

$$P(A\cap B \vert C) = P(A \vert C)P(B \vert C).$$

<figure>
<img src="/assets/pics/mm-pp/cond-ind.png" alt="Conditional independence" style="width: 60%; height: 60%">
<figcaption>Conditional independence
</figcaption>
</figure>

**Remark.** Independence $\nRightarrow$ conditional independence

---

#### (PP 2.3) Independence (continued)

**Definition.**
Eventa $A_1, A_n, \ldots$ are *(mutually) independent* if for any finite $S \subset \\{1, \ldots, n\\}$,

$$P(\cap_{i \in S} A_i) = \prod_{i \in S} P(A_i).$$

**Definition.** $A_1, \ldots, A_n$ are *conditionally independent given $C$* (where $P(C) >0$) if 

$$P(\cap_{i \in S} A_i\vert C) = \prod_{i \in S} P(A_i\vert C).$$

**Proposition.** Suppose $P(B)>0$. Then $A, B$ are independent iff $P(A\vert B) = P(A)$.

<figure>
<img src="/assets/pics/mm-pp/proof-ind.png" alt="Proof of independence" style="width: 90%; height: 90%">
<figcaption>Proof
</figcaption>
</figure>
 
**Exercise.**
<figure>
<img src="/assets/pics/mm-pp/exercise-ind.png" alt="Exercise: independence" style="width: 90%; height: 90%">
<figcaption>
</figcaption>
</figure>

---

#### (PP 2.4) Bayes' rule and the Chain rule

<span style="color:orange">**3 rules: Bayes', Chain, Partition**</span>

**Remark.** $P(A \cap B) = P(A \vert B)P(B)$ if $P(B) >0$

**Theorem (Bayes' rule)**

$$P(B \vert A) = \frac{P(A\vert B)P(B)}{P(A)}$$ 

if $P(A), P(B) >0$.

Plays an important role in particulat in Bayesian statistics.

**Theorem (Chain rule)** If $A_1, \ldots, A_n$ satisfy $P(A_1 \cap \cdots \cap A_n) >0$, then

$$P(A_1 \cap \cdots \cap A_n)
=  P(A_1)P(A_2 \vert A_1)P(A_3\vert A_1\cap A_2)\cdots P(A_n\vert A_1 \cap \cdots \cap A_{n-1}).$$

Proof. By induction.
<figure>
<img src="/assets/pics/mm-pp/proof-chain.png" alt="Proof: The chain rule" style="width: 90%; height: 90%">
<figcaption>
</figcaption>
</figure>

---

#### (PP 2.5) Partition rule, conditional measure

**Definition** A *partition* of $\Omega$ is a nonempty (finite or countable) collection $\\{B_i\\} \subset 2^\Omega$ s.t.
1. $\cup_i B_i = \Omega$
2. $B_i \cap B_j = \emptyset$ if $i \neq j$

**Theorem (Partition rule)** $P(A) = \sum_i P(A \cap B_i)$ for any partition $\\{B_i\\}$ of $\Omega$

Proof: $A = A \cap \Omega = A \cap (\cup_i B_i) = \cup_i (A \cap B_i)$  
$P(A) = P(\cup_i (A\cap B_i)) = \sum_i P(A \cap B_i)$

**Definition.** If $P(B) >0$, then $Q(A) = P(A \vert B)$ defines a probability measure $Q$ (*conditional probability measure given $B$*).

**Exercise.**
* Bayes' rule for cond. prob. meas.:
* Chain rule for cond. prob. meas.:
* Partition rule for cond. prob. meas.:

---

#### (PP 3.1) Random Variables - Definition and CDF

---

$$ $$

*To be added..*

---


