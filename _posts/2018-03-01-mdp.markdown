---
layout: post
title:  "MDP"
date:   2018-03-01 00:00:00
author: 장승환
categories: 기계학습
tags: 강화학습 RL
---

논의를 시작하기 전에 지난번 살펴보았던 확률변수의 개념에 대해 몇 가지 언급해야 할 사항이 있다.


첫째, 지난 포스트 [/확률변수를 이해하다/](https://cveai.github.io/%EA%B8%B0%EA%B3%84%ED%95%99%EC%8A%B5/2018/02/14/rvariable.html)에서는 
이산 확률변수 중에서도 표본공간이 **유한**(finite)인 경우만을 다루었다. 
유한인 경우의 모든 내용이 무한이면서 이산인 경우로 일반화되는데, 달라지는 거의 유일한 포인트는 극한 개념을 도입해야 한다는 것이다. 
극한의 개념이 요구되는 이유를 두가지로 생각해볼 수 있다. 
* 확률변수의 기반이 되는 표본공간이 무한일 경우. 
* 새로운 확률변수를 무한개 확률변수의 합으로 정의할 때.

표본공간이 무한인 이산의 경우는 본질적으로 각 항이 0보다 크거나 같고 $(a_n \ge 0)$ 급수가 1로 수렴하는 $(\sum_{n=1}^\infty a_n = 1)$ 무한수열 $a_n$을 제시하는 것과 동일하다.
여기서 $f(n) = a_n$ 으로 정의하면 $\Omega = \\{1, 2, ... \\}$를 표본공간으로 하는 확률분포함수를 얻게된다.

<figure>
<img src="/assets/pics/mdp/one_over_n.png" alt="Dice" style="width: 80%; height: 80%">
<figcaption>무한 이산 확률분포함수 (using: <a href="https://www.desmos.com/calculator/2kmx0enkkz">Desmos online calculator</a>)
</figcaption>
</figure>

이 확률분포함수에 관한 확률변수 $X: \\{1, 2, \ldots, \\} \rightarrow \mathbb{R}$를 생각해랗 때 달라지는 점은 $X$의 정의구역인 표본공간 $\Omega$이 무한이라는 점이다.

무한개의 활률변수를 합하는 경우는 MDP의 개념을 이해해가면서 구체적으로 다루기로 하자. 

둘째, 확률변수의 공변역은 반드시 실수의 집합이어야 하는 것은 아니다.
확률변수라는 함수의 역할을 살펴보면 정의구역인 표본공간의 확률분포에 관련된 시행이 일어났을 때 확률적으로 어떤 값을 취하게 된다.
따라서 그 값이 꼭 실수일 필요는 없다는 것을 어렵지 않게 이해할 수 있을 것이다.
세상의 대부분의 정보 혹은 데이터를 (적어도 이론적으로는) 수로 표현할 수 있다는 사실을 상기하면 확률변수의 함숫값이 실수인 경우만을 고려해도 큰 문제는 없지만
실수가 아닌 수학적 대상 혹은 다른 어떤 집합을 공변역으로 허용하면 수학적 모델을 기술하는데 (인지학적으로?) 확실히 편리한 면이 있다. 

마지막으로, 확률변수에 다른 함수를 합성하는 것이 자연스럽게 새로운 확률변수를 만드는 방법이라는 것이다.
(정의구역이 표본공간인 함수는 모두 확률변수라는 점을 상기하자!)

 
요약하자면,
* 무한인 경우도 이산일 경우 급수의 극한 걔념을 이용하여 유한의 경우와 매우 유사하게 확률변수를 다룰 수 있다.
* 수가 아니어도
* 덧셈 및 합성


#### 1. 마코프 모델 (Markov Model)

이산적으로 진행되는 시간의 변화에 따라 형성된 데이터를 수학적으로 기술할 때 (거의 필수적으로) 사용되는 것이 마코프 모델이다.
물론 시간순의 데이터를 모델링하는 중요한 이유 중 하나는 (미래에 대한) 예측이다.
시간이 $t = 0, 1, 2, \ldots$ 흐른다고 했을 때 각 시점 $t$에 대응되는 확률적 데이터를 $X_t$라는 어떤 확률변수로 모델링하는 것이 자연스럽다.
이렇게 $X_0, X_1, \ldots$ 라는 일련의 확률변수를 얻게 된다.
이떄 각 확률변수 $X_i$ 사이의 관계를 어떻게 설정해야 할까?

마코프 모델은 이런 상황에서 확률변수들간의 상호관계가 특정한 조건을 만족하는 상황을 모델링한다.
마코프 성질이라 불리는 이 조건이 말하는 것은 다음과 같다:

"Future is independent of the past, given the present."

좀 더 엄밀하게 말하자면, "현재" 시점을 $t$라고 할 때 "미래" 시점에 확률적으로 형성되는 데이터 $X_{t+1}, X_{t+2}, \ldots$는 단지 현시점의 데이터 $X_t$에만 의존한다.
즉 $\ldots, X_{t-2}, X_{t-2}$ 와 독립이다.
 


[Origin of Markov chains](https://youtu.be/Ws63I3F7Moc)

* Markov chain
* Markov Process
* MRP
* MDP  



개념 한줄 요약 정리:
* Markov model : 
* Markov chain :
* Markov process :
* Markov reward process :
* Markov decision process :
* Markov property :
* Hidden Markov :




#### 2. MDP의 요소들

상황설명


확률변수로 바로 들어감






MDP의 개념은 다음의 다섯가지의 기본 요소의 역할을 명확히 이해하는 것이 중요하다.

* Agent
* Environment
* Action
* State
* Reward 


MDP 모델을 조금 쉽게 이해하기 위해 두 주체 간의 게임으로 생각해보자.
이 게임은 주인공인 agent와 주인공이 모험을 할 수 있도록 배경을 설정해주는 environment 간의 상호작용이다.
게임은 이산적인 time step (편의상 시점이라고 부르겠다) 을 따라가며 상호작용이 단계적으로 이루어진다.
좀 더 정확하게 표현해보면 시점을 나타내는 파라미터 $t$가 $t=0$, $t=1$, $t=2$, $\ldots$ 이렇게 변해가면서
agent와 environment가 각자의 활동을 하며 상호작용을 이어가게 된다.
각 시점에서 agent가 할 수 있는 선택을 action이라고 한다.
Agent가 게임에서 취할 수 있는 모든 가능한 action을 다 모아놓은 집합을 $\mathscr{A}$라고 하고, 
시점 $t$에 취하는 action을 $A_t$라고 표시하자. 자연스럽게 $A_t \in \mathscr{A}$임을 알 수 있다.
반면, 각 시점에서 environment는 두가지 반응으로 agent의 action에 대응하는데 그것이 각각 state와 reward이다.
Action의 경우와 마찬가지로 시점 $t$에 environment가 내놓게 되는 state를 $S_t$, reward를 $R_t$로 표시한다.
마찬가지로 $\mathscr{S}$와 $\mathscr{R}$는 각각 가능한 state와 reward의 집합을 나타낸다.







#### 3. 확률변수 $A_t, S_t, R_t$

이제까지 살펴본 내용을 확률변수의 관점에서 정리해보자.

확률적인 과정 (stochastic process) 시점 $t$가 지남에 따라 어떤 데이터들이 확률적으로 생성




#### 4. MDP에 기반한 강화학습 문제, 그리고 Policy

이제 MDP 라는 프레임웍을 가지고 어떤 문제를 제시하고 해결할 수 있는지 살펴보기 시작하자.


#### 5. 가치함수의 가치



#### 6. 강화학습으로 푸는 MDP 문제



#### 7. 그리드 월드 (Grid world) - optional



#### 참고자료

[1] R. Sutton, A. Barto, *Reinforcement leaning: an introduction*, second edition ([final draft](http://incompleteideas.net/book/the-book-2nd.html)).  
[2] D. Silver, *RL Course, Lecture 2: Markov Decision Process* ([유튜브 동영상](https://youtu.be/lfHX2hHRMVQ))   
[3] Mathematical monk, *Probability primer* ([유튜브 동영상](https://www.youtube.com/watch?v=Tk4ubu7BlSk&list=PL17567A1A3F5DB5E4)).  


---

*읽으시다 오류나 부정확한 내용을 발견하시면 꼭 알려주시길 부탁드립니다. 감사합니다.*  
