---
layout: post
title:  "열개의 팔을 가진 강도"
date:   2018-01-31 00:00:00
author: 장승환
categories: 기계학습
tags: 강화학습 RL
---

"카지노에 가본 적이 있으신가요?" 이 질문에 "그렇습니다!" 하고 답할 사람은 많지 않을 것 같지만 "카지노 로얄"  같은 영화를 본 경험이 있다면 카지노 풍경을 상상하는 것은 그리 어렵지 않을 것이다. 

<figure>
<img src="https://cveai.github.io/assets/casino.jpg" alt="Casino Royale">
<figcaption><center>카지노 풍경 (사진 출처: <a href="https://www.flickr.com/photos/prayitnophotography/4464000634">flickr</a>)</center>
</figcaption>
</figure>

슬롯머신을 머릿속에 그려보자. 먼저 제일 간단한 경우로, 레버을 당기면 (실제로 그럴리는 없지만) 항상 같은 금액이 나온다고 가정해보자. 이런 경우 슬롯머신의 reward(보상)의 분포가 deterministic 하다고 말한다. 레버를 당기기 전에 이미 어떤 금액이 나올지 결정되어 있다는 뜻이다. 이런 경우 한번만 당겨보면 이 슬롯머신에 대해 필요한 모든 정보를 아는 샘이 된다. 당연히 카지노 측에서 이렇게 만들어 놓을 리가 없다. 실제로는 어떤 확률 분포를 정하고 거기에 맞추어 보상이 결정되도록 프로그램되어 있을 것이다.

슬롯머신의 레버를 당기면  기대값이 0이고 분산이 1인 정규분포(Gaussian distribution, normal distribution)에 따라 숫자를 토해내는 이상적인 상황을 생각자. 여기서 평균이 0인 분포를 따른다는 것은, 이 슬롯머신의 레버를 당기는 시행을 반복하면서 나오는 숫자들의 평균을 계산해보면 그 횟수가 많아질 수록 평균이 점점 0에 가까워질 것이라는 것을 의미한다. 

한편, 분산은 주어진 분포에 따라 수치를 추출하는 시행을 반복할 때 기대값에 가까운 값이 나올 가능성의 정도를 나타낸다고 볼 수 있다. 예를 들어 아래 그림으로 표현된 두 분포에 의거해 숫자를 출력해보면 분산이 상대적으로 작은 왼쪽 분포(분산 0.9)의 경우 인 0에 가까운 값이 자주 출력되는 반면, 분산이 큰 오른쪽 분포(분산 2.5)의 경우 평균에서 먼 값들도 유사한 빈도로 출력된다.

<figure>
<img src="https://cveai.github.io/assets/var.png" alt="variance">
<figcaption><center>그림 1. 분산 (using: <a href="https://www.desmos.com/calculator/2kmx0enkkz">Desmos online calculator</a>)</center></figcaption>
</figure>

또 한가지 생각해볼 수 있는 것은, 분산이 작은 경우 반복 시행의 평균을 통해 상대적으로 빠르게 기대값에 접근해갈 것이라는 사실이다. (물론 분산이 큰 경우에도 상대적으로 속도는 느리겠지만 어쨌든 기대값에 접근해간다.) 이 원리를 이용하면, 슬롯머신이 정규분포를 따르지만 그 기대값아 알려져있지 않은 경우에도 여러번 반복 시행을 통해서 그 분포의 기대값이 얼마인지를 추정해갈 수 있다. 극단적인 경우로 분산이 0인 다음 그림의 분포는 "항상" 그리고 "반드시" 기대값인 0을 출력하게된다.

*그래프*

#### 열개의 팔을 가진 강도

Multi-armed bandit. 팔 여러개 달린 강도란 뜻이다. 카지노의 슬롯머신을 가리켜 one armed bandit, 즉 “팔하나 달린 강도”라고 부르는 데서 유래한 용어로, "사실상" 돈을 강탈해간다는 의미에서 그러하다. 

<figure>
<img src="https://cveai.github.io/assets/one-armed-bandit.jpg" alt="One-armed banit">
<figcaption><center>One-armed bandits (사진 출처: <a href="https://commons.wikimedia.org/wiki/File:One-Armed_Bandits_at_Stockmen%27s_Hotel,_Elko,_Nevada_(83581).jpg">wikipedia</a>)</center></figcaption>
</figure>



이제 1부터 10까지 번호가 붙여진 열개의 슬롯머신이 놓여있는 방을 머릿속에 떠올려보자. 기대값이 0이고 분산이 1인 표준정규분포에 따라 숫자 열개를 추출하자: $m_1, m_2, \ldots, m_{10}$. 각 슬롯머신의 reward 분포는 기대값이 $m_i$이고 분산이 1인 정규분포에 따르도록 설정이 된다. 아래 그림에 reward 분포가 도식화되어 있는데 (조금 후에 설명할 어떤 이유 때문에) 그림에서는 $i$번째 머신의 기대값 $m_i$가 $q_\*(i)$로 표시되어 있다.

<figure>
<img src="https://cveai.github.io/assets/rew-dist.png" alt="reward distribution">
<figcaption><center>그림 2. (출처: [1], 2장)</center></figcaption>
</figure>

Multi-armed bandit (여기서는 구체적으로 10-armed bandit) 문제는 슬롯머신들의 reward distribution을 모르는 상태에서 선택한 하나의 머신 "팔"을 당겨 reward을 얻는 시행을 충분히 많은 횟수로 반복하면서 얻게되는 reward의 총합을 최대화 시키야하는 문제다. 이 문제를 풀어나가기 위해 수많은 횟수의 시행을 반복하는 힘든 임무를 대신해 줄 주인공을 agent라 부르자. Agent가 선택한 슬롯머신의 레버를 당기는 각 시점을 time step이라 부르고 열 개의 슬롯머신 중에서 하나를 "선택"해서 레버를당기는 행위를 action이라 한다. 선택적 요소가 포함되면서 agent가 어떤 action을 취했을 때 어떤 reward가 주어질 것인지에 대한 “가능성”을 평가 (evaluation) 해나가야 하고 그에 따라 어떤 패턴으로 행동해나가야 하는지 전략을 세워나야야 함을 알 수 있다. 이 전략을 구체적으로 표현한 함수를 policy라 부른다. 우리말로는 “정책” 정도로 부를 수 있을 것이다. 어떤 time step (앞으로 "시간"이라고 부르겠다) $t$에서 agent가 취하는 action을 $A_t$, 그로 인해 얻게 되는 reward를 $R_t$로 부르기로 하자. 

### Action의 가치를 어떻게 평가할 것인가

슬롯머신들의 reward 분포가 그림 1 처럼 주어져있다고 하자. 먼저 우리가 각 머신들의 reward 분포를 전부 알아버린 상황을 가정해보자. 그럼 우리는 agent로 하여금 어떤 선택을 하도록 해야할까? 그렇다. 당연히 3번 슬로머신의 팔만 계속 당기도록 명령해야한 한다. 3번 선택하는 시행을 반복할 수로록 reward의 누적합은 그 기대값인 대략 1.5에 가까워져갈 것임에 비해 다른 아홉개의 머신들의 경우 그보다 작은 값으로 수렴해갈 것이기 때문이다. 그러면 이 문제를 햐결하는 열쇠는 가장 큰 기대값의 reward 분포를 가지는 슬롯머신이 3번이라는 것을 최대한 빨리 알아내는 일일 것이다. 즉, 3번 슬롯머신을 선택하는 action의 가치(value)가 가장 높다는 것을 파악하는 것이 문제의 핵심인 것이다. Reward 분포을 모르는 상태에서 어떻게 이것이 가능할까?

자, agent를 통해 우리가 할 수 있는 일이란 각 시간 $t$에 하나의 action $A_t$를 취해서 그에 따르는 reward $R_t$를 받아 확인 하는 것이 전부다. 그렇다면 슬롯머신들의 reward 분포를 알아내기 위해서 제일 처음 해야할 일은 일단 아무거나 혹은 마음에 드는 번호의 머신을 골라 당겨보는 것일 것이다. 그로 인해 받게 되는 reward 값이 해당 분포의 기대값과 얼마나 일치할지를 알 수는 없지만 그래도 어느 정도 가까운 값일 거라고 기대해 볼 수는 있을 것이다.

각 action의 가치를 평가하고 점점 더 정확해질 수 있도록 업데이트 해나가기 위해 가치함수(action-value function) $Q_t(a)$를 생각한다. $Q_t(a)$는 시간 t에서 $a$라는 action을 취하는 것의 가치를 수치로 표현한 것으로 생각하면 되겠다. 수치가 클수록 $t$라는 시간에 $a$라는 action을 취하는 것의 가치가 높을 것이라고 예상됨을 나타. 한편 우리가 모든 슬롯머신의 reward 분포를 알았다고 가정할 때 각 action에 부여할 만한 가치를 생각해보자. 잠시 생각을 해보면 reward 분포의 기대값이 높을수록 해당 action을 취하는 것이 더 높은 가치가 있다는 것을 알 수 있을 것이다. 따라서 각 action의 $a_i$의 진정한 가치를 $q_{* }(a)=m_i$로 정의하는 것이 꽤 자연스러울 수 있음을 알 수 있을 것이다.

이제 남은것은 구체적으로 $Q_t(a)$라는 함수를 정의하는데 이것이 최대한 적은 시간은 소모하여 $q_{* }(a)$에 접근하도록 잘 고안해야 할 것이다. 충분히 큰 $t$에 대해 함수값들이 충분히 근접했다면 $Q_t(a_3)$의 수치가 가장 클 것임을 알 수 있다. 따라서, 가치함수 $Q_t(a)$를 어떻게 잘 정의하는가가 bandit 문제를 푸는 핵심이라고 하겠다.

### Exploitation vs. Exploration

여태까지의 논의를 요약해보면, 10-armed bandit 문제를 해결하는 핵심은 가장 큰 가치$ q_{* }(a_i)$를 가지는 action $a_i$에 해당하는 슬롯머신을 최대한 빨리 알아내는 것이다. 그 이후에는 오로지 $a_i$라는 action만 취하면 얻는 reward의 총합을 최대화할 수 있을 것이다. 일반적으로 현재 가진 정보로 판단할 때 최대 가치를 가지는 action"만"을 선택해나가는 전략을 exploitation이라고 한고 그러한 개별 행동을 greedy action이라고 한다. 상황에 따라서 greedy action은 눈앞의 이익만 쫓는 탐욕스런 행동이라 할 수 있겠다. 그럼 최적의 action을 판단하기에 충분하지 못한 상황에서 각 머신들의 reward 분포레 대한 정보를 얻어가기 위해서는 어떤 전략을 취해야 할까?


먼저 agent가 각 시간 t에서 몇번 슬롯을 누를지 미리 프로그래밍이 되어있다고 해보자. (수학적으로는 $A_\bullet: \{0, 1, 2, \ldots\} \rightarrow \{a_1,a_2, \ldots ,a_{10}\}$ 이라는 함수를 정의하는 것과 같은 의미다.)


### 일반화된 강화학습 문제

Multi-armed bandit 문제는 강화학습 문제 중에서도 한가지 요소가 극도로 단순화된 형태로 이를 nonassociative 라는 형용사로 표현한다. Associate가 “연결하다” 혹은 “연상시키다”라는 의미를 가지는 동사인데, 보다 일반적인 강화학습 문제에서는 state라는 요소가 도입되어 시간이 지남에 따라 agent의 action에 따라 state가 변화되는 가능성이 포함되어있다. 각 state 마다 agent가 취할 수 있는 action의 선택사양이 달라지기도 한다. 보다 복잡해지고 일반화된 형태의 강화학습 문제를 associative 하다고 표현한다. MAB 문제와 관련하여 예를 든다면 몇번 슬론머신을 선택하느냐에 따라 새로운 방으로 이동하여 슬롯머신이 아닌 다른 게임에 따른 action을 취하고 그로 인해 또 다른 방으로 이동하게 되고.. 이런 식으로 계속 진행된다.

일반화된 강화학습의 개념은 agent와 environment, 그리고 action, state, reward, policy라는 주요소들이 포함된 형태로 정의되는데 수학적으로는 MDP (Markov Decision Process)라는 모델로 기술된다. 

### 참고문헌
1.  R. Sutton, A. Barto, *Reinforcement leaning: an introduction*, second edition ([final draft](http://incompleteideas.net/book/the-book-2nd.html)).
