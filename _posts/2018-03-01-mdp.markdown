---
layout: post
title:  "MDP"
date:   2018-03-01 00:00:00
author: 장승환
categories: 기계학습
tags: 강화학습 RL
---

논의를 시작하기 전에 지난번 살펴보았던 확률변수의 개념에 대해 몇 가지 언급해야 할 사항이 있다.


첫째, 지난 포스트 [/확률변수를 이해하다/](https://cveai.github.io/%EA%B8%B0%EA%B3%84%ED%95%99%EC%8A%B5/2018/02/14/rvariable.html)에서는 
이산 확률변수 중에서도 표본공간이 **유한**(finite)인 경우만을 다루었다. 
유한인 경우의 모든 내용이 무한이면서 이산인 경우로 일반화되는데, 달라지는 거의 유일한 포인트는 극한 개념을 도입해야 한다는 것이다. 
극한의 개념이 요구되는 이유를 두가지로 생각해볼 수 있다. 
* 확률변수의 기반이 되는 표본공간이 무한일 경우. 
* 새로운 확률변수를 무한개 확률변수의 합으로 정의할 때.

표본공간이 무한인 이산의 경우는 본질적으로 각 항이 0보다 크거나 같고 $(a_n \ge 0)$ 급수가 1로 수렴하는 $(\sum a_n = 1)$ 무한수열 $a_n$을 제시하는 것과 동일하다.
여기서 $f(n) = a_n$ 으로 정의하면 $\Omega = \\{1, 2, ... \\}$를 표본공간으로 하는 확률분포함수를 얻게된다.

<figure>
<img src="/assets/pics/mdp/one_over_n.png" alt="Dice" style="width: 80%; height: 80%">
<figcaption>무한 이산 확률분포함수 (using: <a href="https://www.desmos.com/calculator/2kmx0enkkz">Desmos online calculator</a>)
</figcaption>
</figure>

이 확률분포함수에 관한 확률변수 $X: \\{1, 2, \ldots, \\} \rightarrow \mathbb{R}$를 생각해랗 때 달라지는 점은 $X$의 정의구역인 표본공간 $\Omega$이 무한이라는 점이다.

무한개의 활률변수를 합하는 경우는 MDP의 개념을 이해해가면서 구체적으로 다루기로 하자. 

둘째, 확률변수는 확률적 성격 혹은 불확실성을 가진 데이터 (측정에 불확실성도 포함)(의미) --> 함숫값이 실수가 아니어도 된다.

마지막으로, 연산 및 합성해도 확률변수
 
요약하자면,
* 무한인 경우도 이산일 경우 급수의 극한 걔념을 이용하여 유한의 경우와 매우 유사하게 확률변수를 다룰 수 있다.
* 수가 아니어도
* 덧셈 및 합성


#### 마코프 모델 (Markov Model)

이산적으로 진행되는 시간의 변화에 따라 형성된 데이터를 수학적으로 기술할 때 (거의 필수적으로) 사용되는 것이 마코프 모델이다.
물론 시간순의 데이터를 모델링하는 중요한 이유 중 하나는 (미래에 대한) 예측이다.
시간이 $t = 0, 1, 2, \ldots$ 흐른다고 했을 때 각 시점 $t$에 대응되는 확률적 데이터를 $X_t$라는 어떤 확률변수로 모델링하는 것이 자연스럽다.
이렇게 $X_0, X_1, \ldots$ 라는 일련의 확률변수를 얻게 된다.
이떄 각 확률변수 $X_i$ 사이의 관계를 어떻게 설정해야 할까?

마코프 모델은 이런 상황에서 확률변수들간의 상호관계가 특정한 조건을 만족하는 상황을 모델링한다.
마코프 성질이라 불리는 이 조건이 말하는 것은 다음과 같다:

"Future is independent of the past, given the present."

좀 더 엄밀하게 말하자면, "현재" 시점을 $t$라고 할 때 "미래" 시점에 확률적으로 형성되는 데이터 $X_{t+1}, X_{t+2}, \ldots$는 단지 현시점의 데이터 $X_t$에만 의존한다.
즉 $\ldots, X_{t-2}, X_{t-2}$ 와 독립이다.
 


[Origin of Markov chains](https://youtu.be/Ws63I3F7Moc)

* Markov chain
* Markov Process
* MRP
* MDP  

#### MDP (MArkov Decision Process)

상황설명


확률변수로 바로 들어감



MDP의 개념은 다음의 다섯가지의 기본 요소를 명확히 이해하면 어렵지 않게 머릿속에 그림을 그릴 수 있다.

* Agent
* Environment
* Action
* State
* Reward 


MDP 모델을 조금 쉽게 이해하기 위해 두 주체 간의 게임으로 이해해보자.
이 게임은 주인공인 agent와 주인공이 모험을 할 수 있도록 배경을 설정해주는 environment 간의 상호작용이다.
게임은 이산적인 time step (편의상 시점이라고 부르겠다) 을 따라가며 상호작용이 단계적으로 이루어진다.
좀 더 정확하게 표현해보면 시점을 나타내는 파라미터 $t$가 $t=0$, $t=1$, $t=2$, $\ldots$ 이렇게 변해가면서
agent와 environment가 각자의 활동을 하며 상호작용을 이어가게 된다.
각 시점에서 agent가 할 수 있는 선택을 action이라고 한다.
Agent가 게임에서 취할 수 있는 모든 가능한 action을 다 모아놓은 집합을 $\mathscr{A}$라고 하고, 
시점 $t$에 취하는 action을 $A_t$라고 표시하자. 자연스럽게 $A_t \in \mathscr{A}$임을 알 수 있다.
반면, 각 시점에서 environment는 두가지 반응으로 agent의 action에 대응하는데 그것이 각각 state와 reward이다.
Action의 경우와 마찬가지로 시점 $t$에 environment가 내놓게 되는 state를 $S_t$, reward를 $R_t$로 표시한다.
마찬가지로 $\mathscr{S}$와 $\mathscr{R}$는 각각 가능한 state와 reward의 집합을 나타낸다.




#### MDP (Markov Decision Process)

자 이제까지의 내용을 종합하여 정리해보자.

확률적인 과정 (stochastic process) 시점 $t$가 지남에 따라 어떤 데이터들이 확률적으로 생성



#### 확률변수 $A_t, S_t, R_t$ 그리고 $G_t$





#### 그리드 월드 (Grid world)



#### MDP에 기반한 강화학습 문제



#### 참고자료

[1] R. Sutton, A. Barto, *Reinforcement leaning: an introduction*, second edition ([final draft](http://incompleteideas.net/book/the-book-2nd.html)).
[2] D. Silver, *RL Course, Lecture 2: Markov Decision Process* ([유튜브 비디오](https://youtu.be/lfHX2hHRMVQ))



[1] A. Shirayaev (translator: D. Chibisov), *Probability- 1*, third edition (2016), Springer.  
[2] 김민형, *확률론의 선과 악: 2. 확률론의 기원* ([네이버티비 동영상](http://tv.naver.com/v/1402550)).  
[3] Mathematical monk, *Probability primer* ([유튜브 동영상](https://www.youtube.com/watch?v=Tk4ubu7BlSk&list=PL17567A1A3F5DB5E4)).  
[4] PennStae, Eberly College of Science, *STAT 414: Probability theory* ([온라인 코스](https://onlinecourses.science.psu.edu/stat414/)).


---

*읽으시다 오류나 부정확한 내용을 발견하시면 꼭 알려주시길 부탁드립니다. 감사합니다.*  
